{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d51096a-4613-4fc6-bd67-6822f060de1c",
   "metadata": {},
   "source": [
    "# Train Segmentation Models, Extract Masks and Filtered Images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da10bfb0-383d-475e-901f-6efd5890f86a",
   "metadata": {},
   "source": [
    "## 1 Train Segmentation Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e1bcc3a-78c2-4902-927f-8b51b90fb9de",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting ultralytics==8.0.58\n",
      "  Downloading ultralytics-8.0.58-py3-none-any.whl.metadata (23 kB)\n",
      "Requirement already satisfied: matplotlib>=3.2.2 in /home/jupyter-yixuan/.local/lib/python3.10/site-packages (from ultralytics==8.0.58) (3.8.0)\n",
      "Requirement already satisfied: numpy>=1.21.6 in /home/jupyter-yixuan/.local/lib/python3.10/site-packages (from ultralytics==8.0.58) (1.26.1)\n",
      "Collecting opencv-python>=4.6.0 (from ultralytics==8.0.58)\n",
      "  Downloading opencv_python-4.10.0.84-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
      "Requirement already satisfied: Pillow>=7.1.2 in /home/jupyter-yixuan/.local/lib/python3.10/site-packages (from ultralytics==8.0.58) (10.0.1)\n",
      "Requirement already satisfied: PyYAML>=5.3.1 in /opt/tljh/user/lib/python3.10/site-packages (from ultralytics==8.0.58) (6.0.1)\n",
      "Requirement already satisfied: requests>=2.23.0 in /opt/tljh/user/lib/python3.10/site-packages (from ultralytics==8.0.58) (2.31.0)\n",
      "Requirement already satisfied: scipy>=1.4.1 in /home/jupyter-yixuan/.local/lib/python3.10/site-packages (from ultralytics==8.0.58) (1.11.3)\n",
      "Requirement already satisfied: torch>=1.7.0 in /home/jupyter-yixuan/.local/lib/python3.10/site-packages (from ultralytics==8.0.58) (2.2.2)\n",
      "Collecting torchvision>=0.8.1 (from ultralytics==8.0.58)\n",
      "  Downloading torchvision-0.19.0-cp310-cp310-manylinux1_x86_64.whl.metadata (6.0 kB)\n",
      "Requirement already satisfied: tqdm>=4.64.0 in /opt/tljh/user/lib/python3.10/site-packages (from ultralytics==8.0.58) (4.65.0)\n",
      "Requirement already satisfied: pandas>=1.1.4 in /home/jupyter-yixuan/.local/lib/python3.10/site-packages (from ultralytics==8.0.58) (2.1.2)\n",
      "Requirement already satisfied: seaborn>=0.11.0 in /home/jupyter-yixuan/.local/lib/python3.10/site-packages (from ultralytics==8.0.58) (0.13.2)\n",
      "Requirement already satisfied: psutil in /opt/tljh/user/lib/python3.10/site-packages (from ultralytics==8.0.58) (5.9.6)\n",
      "Collecting thop>=0.1.1 (from ultralytics==8.0.58)\n",
      "  Downloading thop-0.1.1.post2209072238-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting sentry-sdk (from ultralytics==8.0.58)\n",
      "  Downloading sentry_sdk-2.12.0-py2.py3-none-any.whl.metadata (9.8 kB)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/jupyter-yixuan/.local/lib/python3.10/site-packages (from matplotlib>=3.2.2->ultralytics==8.0.58) (1.1.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/jupyter-yixuan/.local/lib/python3.10/site-packages (from matplotlib>=3.2.2->ultralytics==8.0.58) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/jupyter-yixuan/.local/lib/python3.10/site-packages (from matplotlib>=3.2.2->ultralytics==8.0.58) (4.43.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/jupyter-yixuan/.local/lib/python3.10/site-packages (from matplotlib>=3.2.2->ultralytics==8.0.58) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/tljh/user/lib/python3.10/site-packages (from matplotlib>=3.2.2->ultralytics==8.0.58) (23.2)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/jupyter-yixuan/.local/lib/python3.10/site-packages (from matplotlib>=3.2.2->ultralytics==8.0.58) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/tljh/user/lib/python3.10/site-packages (from matplotlib>=3.2.2->ultralytics==8.0.58) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/jupyter-yixuan/.local/lib/python3.10/site-packages (from pandas>=1.1.4->ultralytics==8.0.58) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/jupyter-yixuan/.local/lib/python3.10/site-packages (from pandas>=1.1.4->ultralytics==8.0.58) (2023.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/tljh/user/lib/python3.10/site-packages (from requests>=2.23.0->ultralytics==8.0.58) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/tljh/user/lib/python3.10/site-packages (from requests>=2.23.0->ultralytics==8.0.58) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/tljh/user/lib/python3.10/site-packages (from requests>=2.23.0->ultralytics==8.0.58) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/jupyter-yixuan/.local/lib/python3.10/site-packages (from requests>=2.23.0->ultralytics==8.0.58) (2024.2.2)\n",
      "Requirement already satisfied: filelock in /home/jupyter-yixuan/.local/lib/python3.10/site-packages (from torch>=1.7.0->ultralytics==8.0.58) (3.13.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/tljh/user/lib/python3.10/site-packages (from torch>=1.7.0->ultralytics==8.0.58) (4.8.0)\n",
      "Requirement already satisfied: sympy in /home/jupyter-yixuan/.local/lib/python3.10/site-packages (from torch>=1.7.0->ultralytics==8.0.58) (1.12)\n",
      "Requirement already satisfied: networkx in /home/jupyter-yixuan/.local/lib/python3.10/site-packages (from torch>=1.7.0->ultralytics==8.0.58) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /opt/tljh/user/lib/python3.10/site-packages (from torch>=1.7.0->ultralytics==8.0.58) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /home/jupyter-yixuan/.local/lib/python3.10/site-packages (from torch>=1.7.0->ultralytics==8.0.58) (2023.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/jupyter-yixuan/.local/lib/python3.10/site-packages (from torch>=1.7.0->ultralytics==8.0.58) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/jupyter-yixuan/.local/lib/python3.10/site-packages (from torch>=1.7.0->ultralytics==8.0.58) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/jupyter-yixuan/.local/lib/python3.10/site-packages (from torch>=1.7.0->ultralytics==8.0.58) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/jupyter-yixuan/.local/lib/python3.10/site-packages (from torch>=1.7.0->ultralytics==8.0.58) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/jupyter-yixuan/.local/lib/python3.10/site-packages (from torch>=1.7.0->ultralytics==8.0.58) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/jupyter-yixuan/.local/lib/python3.10/site-packages (from torch>=1.7.0->ultralytics==8.0.58) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/jupyter-yixuan/.local/lib/python3.10/site-packages (from torch>=1.7.0->ultralytics==8.0.58) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/jupyter-yixuan/.local/lib/python3.10/site-packages (from torch>=1.7.0->ultralytics==8.0.58) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/jupyter-yixuan/.local/lib/python3.10/site-packages (from torch>=1.7.0->ultralytics==8.0.58) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /home/jupyter-yixuan/.local/lib/python3.10/site-packages (from torch>=1.7.0->ultralytics==8.0.58) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/jupyter-yixuan/.local/lib/python3.10/site-packages (from torch>=1.7.0->ultralytics==8.0.58) (12.1.105)\n",
      "Requirement already satisfied: triton==2.2.0 in /home/jupyter-yixuan/.local/lib/python3.10/site-packages (from torch>=1.7.0->ultralytics==8.0.58) (2.2.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/jupyter-yixuan/.local/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.7.0->ultralytics==8.0.58) (12.4.127)\n",
      "Collecting torch>=1.7.0 (from ultralytics==8.0.58)\n",
      "  Downloading torch-2.4.0-cp310-cp310-manylinux1_x86_64.whl.metadata (26 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.7.0->ultralytics==8.0.58)\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nccl-cu12==2.20.5 (from torch>=1.7.0->ultralytics==8.0.58)\n",
      "  Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting triton==3.0.0 (from torch>=1.7.0->ultralytics==8.0.58)\n",
      "  Downloading triton-3.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: six>=1.5 in /opt/tljh/user/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib>=3.2.2->ultralytics==8.0.58) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/tljh/user/lib/python3.10/site-packages (from jinja2->torch>=1.7.0->ultralytics==8.0.58) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/jupyter-yixuan/.local/lib/python3.10/site-packages (from sympy->torch>=1.7.0->ultralytics==8.0.58) (1.3.0)\n",
      "Downloading ultralytics-8.0.58-py3-none-any.whl (486 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m486.8/486.8 kB\u001b[0m \u001b[31m44.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading opencv_python-4.10.0.84-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (62.5 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m62.5/62.5 MB\u001b[0m \u001b[31m35.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading thop-0.1.1.post2209072238-py3-none-any.whl (15 kB)\n",
      "Downloading torchvision-0.19.0-cp310-cp310-manylinux1_x86_64.whl (7.0 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m76.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading torch-2.4.0-cp310-cp310-manylinux1_x86_64.whl (797.2 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m797.2/797.2 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading triton-3.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (209.4 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m209.4/209.4 MB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading sentry_sdk-2.12.0-py2.py3-none-any.whl (301 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m301.8/301.8 kB\u001b[0m \u001b[31m60.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: triton, sentry-sdk, opencv-python, nvidia-nccl-cu12, nvidia-cudnn-cu12, torch, torchvision, thop, ultralytics\n",
      "  Attempting uninstall: triton\n",
      "    Found existing installation: triton 2.2.0\n",
      "    Uninstalling triton-2.2.0:\n",
      "      Successfully uninstalled triton-2.2.0\n",
      "\u001b[33m  WARNING: The scripts proton and proton-viewer are installed in '/home/jupyter-yixuan/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m  Attempting uninstall: nvidia-nccl-cu12\n",
      "    Found existing installation: nvidia-nccl-cu12 2.19.3\n",
      "    Uninstalling nvidia-nccl-cu12-2.19.3:\n",
      "      Successfully uninstalled nvidia-nccl-cu12-2.19.3\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\n",
      "    Found existing installation: nvidia-cudnn-cu12 8.9.2.26\n",
      "    Uninstalling nvidia-cudnn-cu12-8.9.2.26:\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-8.9.2.26\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.2.2\n",
      "    Uninstalling torch-2.2.2:\n",
      "      Successfully uninstalled torch-2.2.2\n",
      "\u001b[33m  WARNING: The scripts convert-caffe2-to-onnx, convert-onnx-to-caffe2 and torchrun are installed in '/home/jupyter-yixuan/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The scripts ultralytics and yolo are installed in '/home/jupyter-yixuan/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed nvidia-cudnn-cu12-9.1.0.70 nvidia-nccl-cu12-2.20.5 opencv-python-4.10.0.84 sentry-sdk-2.12.0 thop-0.1.1.post2209072238 torch-2.4.0 torchvision-0.19.0 triton-3.0.0 ultralytics-8.0.58\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install ultralytics==8.0.58"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a5d4044a-fca0-4525-9022-acc0cbaa297a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.2.76 available ðŸ˜ƒ Update with 'pip install -U ultralytics'\n",
      "Ultralytics YOLOv8.0.58 ðŸš€ Python-3.10.10 torch-2.4.0+cu121 CUDA:0 (Tesla V100-PCIE-16GB, 16151MiB)\n",
      "\u001b[34m\u001b[1myolo/engine/trainer: \u001b[0mtask=segment, mode=train, model=yolov8n-seg.pt, data=/home/jupyter-yixuan/project_machine_learning/config.yaml, epochs=10, patience=50, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=None, exist_ok=False, pretrained=False, optimizer=SGD, verbose=True, seed=0, deterministic=True, single_cls=False, image_weights=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, hide_labels=False, hide_conf=False, vid_stride=1, line_thickness=3, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, fl_gamma=0.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, v5loader=False, tracker=botsort.yaml, save_dir=runs/segment/train7\n",
      "Overriding model.yaml nc=80 with nc=1\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.Conv                  [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.Conv                  [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.C2f                   [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.Conv                  [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.C2f                   [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.Conv                  [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.C2f                   [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.Conv                  [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.C2f                   [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.SPPF                  [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.Concat                [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.C2f                   [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.Concat                [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.C2f                   [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.Conv                  [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.Concat                [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.C2f                   [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.Conv                  [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.Concat                [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.C2f                   [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1   1004275  ultralytics.nn.modules.Segment               [1, 32, 64, [64, 128, 256]]   \n",
      "YOLOv8n-seg summary: 261 layers, 3263811 parameters, 3263795 gradients, 12.1 GFLOPs\n",
      "\n",
      "Transferred 381/417 items from pretrained weights\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/segment/train7', view at http://localhost:6006/\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n",
      "/home/jupyter-yixuan/.local/lib/python3.10/site-packages/ultralytics/yolo/engine/trainer.py:636: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(True):\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n",
      "/home/jupyter-yixuan/.local/lib/python3.10/site-packages/ultralytics/yolo/engine/trainer.py:217: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = amp.GradScaler(enabled=self.amp)\n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01) with parameter groups 66 weight(decay=0.0), 77 weight(decay=0.0005), 76 bias\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/jupyter-yixuan/project_machine_learning/dataset3/labels/train.cache... 395 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 395/395 [00:00<?, ?it/s]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/jupyter-yixuan/project_machine_learning/dataset3/labels/val.cache... 66 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 66/66 [00:00<?, ?it/s]\n",
      "Plotting labels to runs/segment/train7/labels.jpg... \n",
      "Image sizes 640 train, 640 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1mruns/segment/train7\u001b[0m\n",
      "Starting training for 10 epochs...\n",
      "Closing dataloader mosaic\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "  0%|          | 0/25 [00:00<?, ?it/s]/home/jupyter-yixuan/.local/lib/python3.10/site-packages/ultralytics/yolo/engine/trainer.py:317: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(self.amp):\n",
      "       1/10      2.79G      0.665       1.66      2.929       1.14         11        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:04<00:00,  5.08it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  2.30it/s]\n",
      "                   all         66         91    0.00359       0.78      0.579      0.499    0.00348      0.758      0.571      0.485\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       2/10      3.31G     0.5525      1.117      1.807       1.09         11        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:03<00:00,  6.31it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  2.32it/s]\n",
      "                   all         66         91      0.717      0.502      0.522        0.4      0.686       0.48      0.503      0.373\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       3/10      3.31G     0.6534      1.041      1.598      1.153         11        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:03<00:00,  6.38it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  2.32it/s]\n",
      "                   all         66         91      0.616      0.396      0.494      0.386      0.616      0.396      0.491      0.364\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       4/10      3.31G     0.6576      1.156      1.424       1.16         11        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:03<00:00,  6.31it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  2.29it/s]\n",
      "                   all         66         91        0.7      0.563      0.486      0.357      0.686      0.552      0.475      0.337\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       5/10      3.31G     0.6769       1.08      1.358      1.142         11        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:04<00:00,  6.18it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  2.34it/s]\n",
      "                   all         66         91      0.668      0.531      0.517      0.389      0.668      0.531      0.515      0.369\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       6/10      3.31G     0.6882      1.145      1.275      1.159         12        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:03<00:00,  6.47it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  2.30it/s]\n",
      "                   all         66         91      0.767      0.516      0.537      0.419      0.828      0.516      0.563        0.4\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       7/10      3.31G     0.6843      1.124      1.197      1.156         11        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:03<00:00,  6.41it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  2.33it/s]\n",
      "                   all         66         91      0.692      0.571      0.545      0.433      0.692      0.571      0.546      0.401\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       8/10      3.31G     0.6691      1.036      1.147      1.147         12        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:04<00:00,  6.18it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  2.34it/s]\n",
      "                   all         66         91      0.716      0.609      0.592      0.477      0.789      0.538       0.58      0.448\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       9/10      3.31G     0.5958     0.9141      1.063      1.078         11        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:03<00:00,  6.43it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  2.28it/s]\n",
      "                   all         66         91      0.691      0.615       0.54      0.443      0.717      0.604      0.532      0.415\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      10/10      3.31G     0.5675        0.9      1.001      1.061         11        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:03<00:00,  6.39it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  1.51it/s]\n",
      "                   all         66         91       0.73      0.637      0.585      0.483      0.742      0.648      0.592      0.469\n",
      "\n",
      "10 epochs completed in 0.023 hours.\n",
      "/home/jupyter-yixuan/.local/lib/python3.10/site-packages/ultralytics/yolo/utils/torch_utils.py:332: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  x = torch.load(f, map_location=torch.device('cpu'))\n",
      "Optimizer stripped from runs/segment/train7/weights/last.pt, 6.8MB\n",
      "Optimizer stripped from runs/segment/train7/weights/best.pt, 6.8MB\n",
      "\n",
      "Validating runs/segment/train7/weights/best.pt...\n",
      "Ultralytics YOLOv8.0.58 ðŸš€ Python-3.10.10 torch-2.4.0+cu121 CUDA:0 (Tesla V100-PCIE-16GB, 16151MiB)\n",
      "/home/jupyter-yixuan/.local/lib/python3.10/site-packages/ultralytics/nn/tasks.py:336: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(file, map_location='cpu'), file  # load\n",
      "YOLOv8n-seg summary (fused): 195 layers, 3258259 parameters, 0 gradients, 12.0 GFLOPs\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  1.51it/s]\n",
      "                   all         66         91    0.00359       0.78       0.58      0.498    0.00348      0.758      0.571      0.485\n",
      "Speed: 0.2ms preprocess, 1.6ms inference, 0.0ms loss, 3.2ms postprocess per image\n",
      "Results saved to \u001b[1mruns/segment/train7\u001b[0m\n",
      "/home/jupyter-yixuan/.local/lib/python3.10/site-packages/ultralytics/nn/tasks.py:336: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(file, map_location='cpu'), file  # load\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# Load a model\n",
    "model = YOLO(\"yolov8n-seg.pt\")  # load an official model\n",
    "\n",
    "# Train the model\n",
    "results = model.train(data=\"/home/jupyter-yixuan/project_machine_learning/config.yaml\", epochs=10, imgsz=640)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c9f026fd-63d7-4d2e-b936-737083357df5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.2.76 available ðŸ˜ƒ Update with 'pip install -U ultralytics'\n",
      "Ultralytics YOLOv8.0.58 ðŸš€ Python-3.10.10 torch-2.4.0+cu121 CUDA:0 (Tesla V100-PCIE-16GB, 16151MiB)\n",
      "\u001b[34m\u001b[1myolo/engine/trainer: \u001b[0mtask=segment, mode=train, model=yolov8n-seg.pt, data=/home/jupyter-yixuan/project_machine_learning/config.yaml, epochs=30, patience=50, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=None, exist_ok=False, pretrained=False, optimizer=SGD, verbose=True, seed=0, deterministic=True, single_cls=False, image_weights=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, hide_labels=False, hide_conf=False, vid_stride=1, line_thickness=3, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, fl_gamma=0.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, v5loader=False, tracker=botsort.yaml, save_dir=runs/segment/train8\n",
      "Overriding model.yaml nc=80 with nc=1\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.Conv                  [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.Conv                  [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.C2f                   [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.Conv                  [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.C2f                   [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.Conv                  [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.C2f                   [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.Conv                  [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.C2f                   [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.SPPF                  [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.Concat                [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.C2f                   [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.Concat                [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.C2f                   [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.Conv                  [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.Concat                [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.C2f                   [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.Conv                  [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.Concat                [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.C2f                   [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1   1004275  ultralytics.nn.modules.Segment               [1, 32, 64, [64, 128, 256]]   \n",
      "YOLOv8n-seg summary: 261 layers, 3263811 parameters, 3263795 gradients, 12.1 GFLOPs\n",
      "\n",
      "Transferred 381/417 items from pretrained weights\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/segment/train8', view at http://localhost:6006/\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n",
      "/home/jupyter-yixuan/.local/lib/python3.10/site-packages/ultralytics/yolo/engine/trainer.py:636: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(True):\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n",
      "/home/jupyter-yixuan/.local/lib/python3.10/site-packages/ultralytics/yolo/engine/trainer.py:217: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = amp.GradScaler(enabled=self.amp)\n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01) with parameter groups 66 weight(decay=0.0), 77 weight(decay=0.0005), 76 bias\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/jupyter-yixuan/project_machine_learning/dataset3/labels/train.cache... 395 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 395/395 [00:00<?, ?it/s]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/jupyter-yixuan/project_machine_learning/dataset3/labels/val.cache... 66 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 66/66 [00:00<?, ?it/s]\n",
      "Plotting labels to runs/segment/train8/labels.jpg... \n",
      "Image sizes 640 train, 640 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1mruns/segment/train8\u001b[0m\n",
      "Starting training for 30 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "  0%|          | 0/25 [00:00<?, ?it/s]/home/jupyter-yixuan/.local/lib/python3.10/site-packages/ultralytics/yolo/engine/trainer.py:317: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(self.amp):\n",
      "       1/30      2.89G     0.6488      1.615      2.381      1.093         29        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:06<00:00,  3.78it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  2.18it/s]\n",
      "                   all         66         91     0.0082      0.758       0.53      0.465    0.00785      0.725      0.529      0.453\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       2/30       3.4G     0.5875       1.06      1.458      1.043         28        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:05<00:00,  4.92it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  2.27it/s]\n",
      "                   all         66         91      0.705      0.319      0.509      0.422      0.705      0.319      0.508      0.419\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       3/30       3.4G     0.6834      1.135      1.305      1.092         23        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:04<00:00,  5.05it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  2.29it/s]\n",
      "                   all         66         91      0.654      0.538      0.519       0.36      0.654      0.538      0.519       0.37\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       4/30       3.4G     0.6782      1.217      1.209      1.082         26        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:04<00:00,  5.07it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  2.23it/s]\n",
      "                   all         66         91      0.693      0.521      0.504      0.348      0.692      0.518      0.491      0.344\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       5/30       3.4G     0.7233      1.149      1.191      1.116         29        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:04<00:00,  5.02it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  2.26it/s]\n",
      "                   all         66         91      0.613      0.571      0.546      0.271      0.533      0.516      0.475      0.253\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       6/30       3.4G     0.7268      1.243      1.178      1.113         23        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:05<00:00,  4.98it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  2.29it/s]\n",
      "                   all         66         91      0.629      0.521      0.533      0.319      0.576      0.492      0.487      0.293\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       7/30       3.4G     0.7192      1.221      1.161      1.108         24        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:04<00:00,  5.01it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  2.27it/s]\n",
      "                   all         66         91      0.571      0.637      0.498      0.353      0.696      0.495      0.478      0.343\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       8/30       3.4G     0.7338      1.165      1.102      1.119         30        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:05<00:00,  4.60it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  2.28it/s]\n",
      "                   all         66         91      0.743      0.593      0.545      0.367      0.733      0.574      0.536      0.391\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       9/30       3.4G      0.754      1.199      1.068      1.127         30        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:05<00:00,  4.76it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  2.26it/s]\n",
      "                   all         66         91      0.762      0.593      0.568      0.394      0.762      0.593      0.573      0.407\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      10/30       3.4G     0.7103      1.148      1.025      1.119         26        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:05<00:00,  4.93it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  2.27it/s]\n",
      "                   all         66         91      0.745      0.582      0.547      0.399      0.745      0.582      0.548       0.39\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      11/30       3.4G     0.7331      1.172      1.037       1.11         31        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:05<00:00,  4.77it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  2.30it/s]\n",
      "                   all         66         91       0.73      0.582      0.565      0.419      0.743      0.593      0.574        0.4\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      12/30       3.4G     0.7221      1.074      1.004       1.09         35        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:05<00:00,  4.73it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  2.29it/s]\n",
      "                   all         66         91      0.686      0.593       0.57      0.437      0.672      0.582       0.56      0.416\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      13/30       3.4G     0.7144       1.05     0.9488       1.09         30        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:05<00:00,  4.78it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  2.28it/s]\n",
      "                   all         66         91      0.707      0.648      0.599      0.448      0.694      0.637      0.588      0.421\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      14/30       3.4G     0.7138      1.129     0.9354      1.101         32        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:04<00:00,  5.02it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  2.30it/s]\n",
      "                   all         66         91      0.611      0.582       0.49      0.324      0.579       0.56      0.465      0.268\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      15/30       3.4G     0.6601     0.9921     0.8904      1.061         25        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:05<00:00,  4.86it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  2.30it/s]\n",
      "                   all         66         91      0.702      0.582      0.517      0.376      0.686      0.571      0.502      0.308\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      16/30       3.4G     0.6635      1.072     0.8436      1.059         28        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:04<00:00,  5.06it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  2.30it/s]\n",
      "                   all         66         91      0.772      0.637      0.621       0.49      0.772      0.637      0.618      0.485\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      17/30       3.4G     0.6428     0.9871     0.8704      1.048         25        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:04<00:00,  5.05it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  2.27it/s]\n",
      "                   all         66         91       0.69      0.661      0.634      0.503      0.702      0.672      0.641      0.502\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      18/30       3.4G     0.6231     0.9735     0.7895      1.047         29        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:05<00:00,  4.78it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  2.31it/s]\n",
      "                   all         66         91      0.753      0.626      0.625      0.516      0.824      0.593      0.625      0.499\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      19/30       3.4G     0.5975     0.8967     0.8041      1.043         21        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:04<00:00,  5.10it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  2.29it/s]\n",
      "                   all         66         91      0.803      0.593       0.63       0.52      0.818      0.604      0.638      0.507\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      20/30       3.4G     0.5989     0.9084     0.7617      1.058         30        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:04<00:00,  5.08it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  2.30it/s]\n",
      "                   all         66         91      0.766      0.626      0.613      0.505      0.766      0.626      0.613      0.497\n",
      "Closing dataloader mosaic\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      21/30       3.4G     0.5513     0.8323     0.9663      1.053         11        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:04<00:00,  5.08it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  2.35it/s]\n",
      "                   all         66         91      0.729      0.615      0.611      0.494      0.732        0.6      0.598       0.47\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      22/30       3.4G     0.5127     0.8015     0.8382      1.023         11        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:04<00:00,  6.09it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  2.27it/s]\n",
      "                   all         66         91      0.719      0.589      0.605      0.514      0.745       0.61      0.608      0.491\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      23/30       3.4G      0.517      0.829     0.7857      1.041         11        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:04<00:00,  6.12it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  2.31it/s]\n",
      "                   all         66         91       0.77      0.664       0.61      0.492       0.77      0.664      0.605      0.484\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      24/30       3.4G     0.4738     0.7453     0.7201     0.9858         11        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:04<00:00,  6.09it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  2.28it/s]\n",
      "                   all         66         91      0.758      0.654      0.634      0.534      0.788       0.67      0.639      0.517\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      25/30       3.4G     0.4507     0.7063     0.6795     0.9889         11        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:04<00:00,  6.23it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  2.33it/s]\n",
      "                   all         66         91      0.726       0.67      0.629      0.536      0.738      0.681      0.639      0.522\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      26/30       3.4G     0.4406     0.6269     0.6713     0.9769         11        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:03<00:00,  6.25it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  2.28it/s]\n",
      "                   all         66         91      0.758      0.659      0.617      0.553      0.758      0.659      0.616      0.507\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      27/30       3.4G     0.4464     0.6685     0.6729     0.9677         11        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:03<00:00,  6.29it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  2.31it/s]\n",
      "                   all         66         91       0.75      0.648      0.617       0.55      0.763      0.659      0.621      0.509\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      28/30       3.4G     0.4054     0.6213     0.6144      0.944         11        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:04<00:00,  6.16it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  2.27it/s]\n",
      "                   all         66         91      0.756      0.637      0.632      0.546       0.77      0.648      0.638       0.53\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      29/30       3.4G     0.3949     0.6475     0.6286     0.9384         11        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:04<00:00,  6.00it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  2.31it/s]\n",
      "                   all         66         91      0.692      0.668      0.624      0.542      0.802      0.626      0.632      0.522\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      30/30       3.4G     0.3944     0.6027      0.591     0.9364         11        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:04<00:00,  6.20it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:02<00:00,  1.46it/s]\n",
      "                   all         66         91      0.753      0.637      0.629      0.551      0.778      0.656       0.64      0.525\n",
      "\n",
      "30 epochs completed in 0.064 hours.\n",
      "/home/jupyter-yixuan/.local/lib/python3.10/site-packages/ultralytics/yolo/utils/torch_utils.py:332: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  x = torch.load(f, map_location=torch.device('cpu'))\n",
      "Optimizer stripped from runs/segment/train8/weights/last.pt, 6.8MB\n",
      "Optimizer stripped from runs/segment/train8/weights/best.pt, 6.8MB\n",
      "\n",
      "Validating runs/segment/train8/weights/best.pt...\n",
      "Ultralytics YOLOv8.0.58 ðŸš€ Python-3.10.10 torch-2.4.0+cu121 CUDA:0 (Tesla V100-PCIE-16GB, 16151MiB)\n",
      "/home/jupyter-yixuan/.local/lib/python3.10/site-packages/ultralytics/nn/tasks.py:336: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(file, map_location='cpu'), file  # load\n",
      "YOLOv8n-seg summary (fused): 195 layers, 3258259 parameters, 0 gradients, 12.0 GFLOPs\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  1.52it/s]\n",
      "                   all         66         91      0.756      0.637      0.632      0.545      0.769      0.648      0.638      0.531\n",
      "Speed: 0.2ms preprocess, 2.2ms inference, 0.0ms loss, 0.8ms postprocess per image\n",
      "Results saved to \u001b[1mruns/segment/train8\u001b[0m\n",
      "/home/jupyter-yixuan/.local/lib/python3.10/site-packages/ultralytics/nn/tasks.py:336: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(file, map_location='cpu'), file  # load\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# Load a model\n",
    "model = YOLO(\"yolov8n-seg.pt\")  # load an official model\n",
    "\n",
    "# Train the model\n",
    "results = model.train(data=\"/home/jupyter-yixuan/project_machine_learning/config.yaml\", epochs=30, imgsz=640)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8709b29b-3eac-4e56-b95b-cedff5d21b9e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.2.76 available ðŸ˜ƒ Update with 'pip install -U ultralytics'\n",
      "Ultralytics YOLOv8.0.58 ðŸš€ Python-3.10.10 torch-2.4.0+cu121 CUDA:0 (Tesla V100-PCIE-16GB, 16151MiB)\n",
      "\u001b[34m\u001b[1myolo/engine/trainer: \u001b[0mtask=segment, mode=train, model=yolov8n-seg.pt, data=/home/jupyter-yixuan/project_machine_learning/config.yaml, epochs=60, patience=50, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=None, exist_ok=False, pretrained=False, optimizer=SGD, verbose=True, seed=0, deterministic=True, single_cls=False, image_weights=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, hide_labels=False, hide_conf=False, vid_stride=1, line_thickness=3, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, fl_gamma=0.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, v5loader=False, tracker=botsort.yaml, save_dir=runs/segment/train9\n",
      "Overriding model.yaml nc=80 with nc=1\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.Conv                  [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.Conv                  [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.C2f                   [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.Conv                  [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.C2f                   [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.Conv                  [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.C2f                   [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.Conv                  [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.C2f                   [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.SPPF                  [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.Concat                [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.C2f                   [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.Concat                [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.C2f                   [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.Conv                  [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.Concat                [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.C2f                   [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.Conv                  [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.Concat                [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.C2f                   [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1   1004275  ultralytics.nn.modules.Segment               [1, 32, 64, [64, 128, 256]]   \n",
      "YOLOv8n-seg summary: 261 layers, 3263811 parameters, 3263795 gradients, 12.1 GFLOPs\n",
      "\n",
      "Transferred 381/417 items from pretrained weights\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/segment/train9', view at http://localhost:6006/\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n",
      "/home/jupyter-yixuan/.local/lib/python3.10/site-packages/ultralytics/yolo/engine/trainer.py:636: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(True):\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n",
      "/home/jupyter-yixuan/.local/lib/python3.10/site-packages/ultralytics/yolo/engine/trainer.py:217: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = amp.GradScaler(enabled=self.amp)\n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01) with parameter groups 66 weight(decay=0.0), 77 weight(decay=0.0005), 76 bias\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/jupyter-yixuan/project_machine_learning/dataset3/labels/train.cache... 395 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 395/395 [00:00<?, ?it/s]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/jupyter-yixuan/project_machine_learning/dataset3/labels/val.cache... 66 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 66/66 [00:00<?, ?it/s]\n",
      "Plotting labels to runs/segment/train9/labels.jpg... \n",
      "Image sizes 640 train, 640 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1mruns/segment/train9\u001b[0m\n",
      "Starting training for 60 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "  0%|          | 0/25 [00:00<?, ?it/s]/home/jupyter-yixuan/.local/lib/python3.10/site-packages/ultralytics/yolo/engine/trainer.py:317: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(self.amp):\n",
      "       1/60       2.9G     0.6488      1.615      2.381      1.093         29        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:06<00:00,  4.06it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  2.23it/s]\n",
      "                   all         66         91     0.0082      0.758       0.53      0.465    0.00785      0.725      0.529      0.453\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       2/60      3.41G      0.587      1.085      1.463      1.041         28        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:05<00:00,  4.65it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  2.23it/s]\n",
      "                   all         66         91      0.737      0.462      0.532      0.462      0.737      0.462      0.532      0.456\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       3/60      3.41G     0.6842      1.139      1.323      1.092         23        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:05<00:00,  4.93it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  2.20it/s]\n",
      "                   all         66         91       0.73      0.536      0.523      0.426       0.73      0.536      0.523      0.408\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       4/60      3.41G     0.6762      1.177      1.248      1.079         26        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:04<00:00,  5.02it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  2.23it/s]\n",
      "                   all         66         91      0.747      0.571      0.576      0.435       0.76      0.582      0.579      0.432\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       5/60      3.41G     0.7225      1.157      1.157      1.094         29        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:05<00:00,  4.79it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  2.27it/s]\n",
      "                   all         66         91      0.728      0.589       0.56      0.385      0.739      0.549      0.521      0.329\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       6/60      3.41G     0.7525      1.365      1.191      1.116         23        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:04<00:00,  5.04it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  2.28it/s]\n",
      "                   all         66         91        0.6      0.484      0.492      0.326      0.562      0.462      0.453      0.234\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       7/60      3.41G     0.7755      1.286       1.21      1.129         24        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:04<00:00,  5.05it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  2.25it/s]\n",
      "                   all         66         91      0.668      0.596      0.542      0.333      0.652      0.571      0.532       0.34\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       8/60      3.41G     0.7665      1.219      1.139      1.152         30        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:05<00:00,  4.82it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  2.27it/s]\n",
      "                   all         66         91      0.675      0.604      0.586      0.418      0.675      0.604      0.579      0.396\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       9/60      3.41G     0.7694      1.296      1.114      1.133         30        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:04<00:00,  5.08it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  2.24it/s]\n",
      "                   all         66         91       0.64      0.585      0.492      0.326      0.628      0.574      0.487      0.295\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      10/60      3.41G     0.7569      1.206      1.063      1.133         26        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:05<00:00,  4.93it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  2.33it/s]\n",
      "                   all         66         91      0.647       0.56      0.533      0.384      0.635       0.56      0.529      0.308\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      11/60      3.41G     0.7653      1.197      1.068      1.118         31        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:04<00:00,  5.01it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  2.29it/s]\n",
      "                   all         66         91      0.765      0.626      0.581      0.398      0.724      0.593      0.534      0.339\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      12/60      3.41G     0.7498      1.222      1.028      1.111         35        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:05<00:00,  4.76it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  2.30it/s]\n",
      "                   all         66         91      0.652      0.596      0.527      0.404      0.663      0.626      0.528      0.372\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      13/60      3.41G     0.7487      1.098     0.9772       1.11         30        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:05<00:00,  4.55it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  2.19it/s]\n",
      "                   all         66         91      0.565      0.585      0.517        0.4      0.565      0.585      0.524      0.378\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      14/60      3.41G     0.7542      1.177     0.9673      1.135         32        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:05<00:00,  4.91it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  2.32it/s]\n",
      "                   all         66         91      0.724      0.582      0.582      0.442      0.724      0.582      0.582      0.438\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      15/60      3.41G     0.6959      1.032     0.9262      1.088         25        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:05<00:00,  4.72it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  2.28it/s]\n",
      "                   all         66         91      0.715       0.56      0.567      0.424      0.715       0.56      0.557      0.385\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      16/60      3.41G     0.7162      1.146     0.8957      1.094         28        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:05<00:00,  4.81it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  2.32it/s]\n",
      "                   all         66         91      0.753      0.637      0.611      0.455       0.74      0.626      0.599      0.453\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      17/60      3.41G     0.6804      1.075     0.8933      1.071         25        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:05<00:00,  4.96it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  2.29it/s]\n",
      "                   all         66         91      0.747      0.604      0.624      0.497      0.734      0.593      0.626      0.443\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      18/60      3.41G     0.6887      1.023     0.8487      1.077         29        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:04<00:00,  5.02it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  2.35it/s]\n",
      "                   all         66         91      0.812      0.538      0.566      0.384      0.762      0.505      0.528      0.322\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      19/60      3.41G     0.6838     0.9834      0.848      1.097         21        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:04<00:00,  5.11it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  2.29it/s]\n",
      "                   all         66         91      0.732      0.631      0.627      0.511      0.732      0.631       0.63      0.496\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      20/60      3.41G     0.6689      1.009     0.7978       1.09         30        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:04<00:00,  5.04it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  2.30it/s]\n",
      "                   all         66         91      0.638      0.615      0.561      0.428      0.626      0.604      0.557      0.406\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      21/60      3.41G     0.6796     0.9871     0.8011      1.076         30        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:04<00:00,  5.00it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  2.32it/s]\n",
      "                   all         66         91      0.708       0.67      0.591      0.482      0.719      0.681      0.595      0.465\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      22/60      3.41G     0.6801      1.004     0.8293      1.063         28        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:05<00:00,  4.93it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  2.31it/s]\n",
      "                   all         66         91      0.701      0.582      0.579      0.453      0.718      0.593      0.584      0.433\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      23/60      3.41G     0.6302     0.9602     0.7478      1.049         30        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:04<00:00,  5.64it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  2.26it/s]\n",
      "                   all         66         91      0.738      0.651      0.593      0.469      0.738      0.651      0.593      0.467\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      24/60      3.41G     0.6365      0.932     0.7666      1.058         29        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:04<00:00,  5.04it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  2.30it/s]\n",
      "                   all         66         91      0.732      0.637      0.601      0.501       0.73      0.637      0.606       0.45\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      25/60      3.41G     0.6173     0.9453     0.7832      1.041         28        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:04<00:00,  5.15it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  2.34it/s]\n",
      "                   all         66         91      0.738      0.593      0.615      0.466      0.751      0.604      0.623      0.415\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      26/60      3.41G     0.6275     0.9607     0.7308      1.054         31        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:04<00:00,  5.04it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  2.32it/s]\n",
      "                   all         66         91      0.682      0.538      0.545      0.388       0.68      0.538      0.533      0.357\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      27/60      3.41G      0.596     0.9592     0.7225      1.039         26        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:04<00:00,  5.24it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  2.33it/s]\n",
      "                   all         66         91      0.719      0.604      0.569       0.47      0.719      0.604      0.569       0.44\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      28/60      3.41G     0.6205     0.9871     0.7283      1.038         28        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:04<00:00,  5.00it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  2.28it/s]\n",
      "                   all         66         91      0.774      0.604      0.597      0.498      0.789      0.615      0.605       0.47\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      29/60      3.41G     0.6012     0.8827     0.6828      1.026         21        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:05<00:00,  4.82it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  2.33it/s]\n",
      "                   all         66         91      0.691      0.664      0.591      0.479       0.68      0.653      0.588       0.47\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      30/60      3.41G     0.5816     0.8745      0.709      1.023         24        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:05<00:00,  4.65it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  2.26it/s]\n",
      "                   all         66         91      0.714      0.658      0.598      0.508      0.714      0.658      0.595      0.482\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      31/60      3.41G     0.6032      0.849     0.6835      1.028         31        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:05<00:00,  4.84it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  2.30it/s]\n",
      "                   all         66         91      0.752      0.648      0.593      0.509      0.752      0.648      0.593      0.482\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      32/60      3.41G     0.5711      0.858     0.6787      1.014         25        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:04<00:00,  5.21it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  2.29it/s]\n",
      "                   all         66         91       0.77      0.615      0.609      0.505      0.785      0.637       0.62      0.495\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      33/60      3.41G     0.5545     0.8951     0.6455      1.004         30        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:05<00:00,  4.85it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  2.29it/s]\n",
      "                   all         66         91      0.747      0.626      0.603      0.503       0.76      0.637      0.599      0.475\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      34/60      3.41G     0.5699     0.7872     0.6546     0.9953         28        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:04<00:00,  5.10it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  2.32it/s]\n",
      "                   all         66         91      0.747       0.67      0.606      0.499      0.736      0.681      0.595       0.47\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      35/60      3.41G     0.5503     0.8208     0.6268     0.9953         24        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:04<00:00,  5.14it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  2.32it/s]\n",
      "                   all         66         91      0.733      0.635      0.609      0.516      0.746      0.646      0.612      0.495\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      36/60      3.41G     0.5765      0.853     0.6619      1.018         35        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:05<00:00,  4.94it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  2.31it/s]\n",
      "                   all         66         91      0.728      0.637      0.611      0.512       0.74      0.648      0.617      0.508\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      37/60      3.41G      0.557     0.8018     0.6383      1.004         24        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:04<00:00,  5.01it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  2.33it/s]\n",
      "                   all         66         91      0.813      0.626      0.627      0.512      0.827      0.637      0.634      0.515\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      38/60      3.41G     0.5437     0.7516     0.6348     0.9936         28        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:05<00:00,  5.00it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  2.30it/s]\n",
      "                   all         66         91      0.734      0.648      0.607      0.512      0.746      0.659      0.611      0.495\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      39/60      3.41G     0.5505     0.7424     0.6173      1.007         30        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:05<00:00,  4.97it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  2.32it/s]\n",
      "                   all         66         91      0.729      0.637      0.608       0.51      0.742      0.648      0.609      0.495\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      40/60      3.41G     0.5179     0.7777     0.5937     0.9766         23        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:05<00:00,  4.78it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  2.32it/s]\n",
      "                   all         66         91      0.757      0.616      0.631      0.544       0.77      0.627      0.635       0.52\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      41/60      3.41G     0.5028     0.7688     0.5681     0.9763         30        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:04<00:00,  5.26it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  2.31it/s]\n",
      "                   all         66         91      0.733      0.648      0.626      0.541      0.733      0.648      0.629      0.522\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      42/60      3.41G     0.4974     0.7432     0.5502     0.9893         37        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:05<00:00,  4.80it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  2.34it/s]\n",
      "                   all         66         91      0.693      0.648      0.625      0.515       0.71      0.648      0.633       0.51\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      43/60      3.41G     0.5015     0.7559     0.5685     0.9813         31        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:05<00:00,  4.53it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  2.31it/s]\n",
      "                   all         66         91       0.75      0.615      0.635      0.555       0.75      0.615      0.634      0.517\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      44/60      3.41G     0.5106     0.7397     0.5764     0.9916         31        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:05<00:00,  4.46it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  2.29it/s]\n",
      "                   all         66         91      0.738       0.68      0.635      0.558      0.794      0.615       0.63      0.525\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      45/60      3.41G     0.5154     0.7325     0.5806     0.9935         27        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:04<00:00,  5.22it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  2.34it/s]\n",
      "                   all         66         91      0.758      0.637      0.649      0.564      0.788      0.626      0.655      0.524\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      46/60      3.41G     0.5021     0.7076     0.5495     0.9732         20        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:05<00:00,  4.97it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  2.34it/s]\n",
      "                   all         66         91      0.735      0.626      0.626      0.542      0.761      0.648      0.637      0.523\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      47/60      3.41G      0.487     0.6689     0.5329     0.9663         34        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:05<00:00,  4.90it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  2.33it/s]\n",
      "                   all         66         91       0.77      0.624      0.605      0.518      0.783      0.635      0.611      0.502\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      48/60      3.41G     0.4679     0.6987     0.5237     0.9497         25        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:04<00:00,  5.02it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  2.34it/s]\n",
      "                   all         66         91      0.813      0.574      0.606      0.524      0.804      0.615      0.613      0.504\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      49/60      3.41G     0.5034     0.7378     0.5545     0.9856         29        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:05<00:00,  4.89it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  2.33it/s]\n",
      "                   all         66         91      0.802      0.593      0.617      0.535      0.817      0.604      0.618      0.518\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      50/60      3.41G     0.4628     0.6724     0.4984     0.9632         25        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:05<00:00,  4.93it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  2.32it/s]\n",
      "                   all         66         91      0.764      0.626      0.633      0.551      0.791      0.648      0.645      0.524\n",
      "Closing dataloader mosaic\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      51/60      3.41G     0.3994     0.5939     0.5788     0.9528         11        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:04<00:00,  5.17it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  2.29it/s]\n",
      "                   all         66         91      0.774      0.615      0.613      0.531      0.761      0.628      0.623      0.503\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      52/60      3.41G     0.3693     0.5381     0.4947     0.9214         11        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:04<00:00,  6.20it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  2.30it/s]\n",
      "                   all         66         91      0.756      0.659      0.593      0.528      0.781      0.681      0.618      0.501\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      53/60      3.41G     0.3654     0.5711     0.4453     0.9175         12        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:03<00:00,  6.28it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  2.30it/s]\n",
      "                   all         66         91      0.786      0.659      0.621      0.555      0.799       0.67      0.632       0.52\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      54/60      3.41G     0.3664     0.5506     0.4495     0.9262         11        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:04<00:00,  6.19it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  2.29it/s]\n",
      "                   all         66         91      0.792      0.659      0.621      0.548      0.792      0.659      0.616      0.508\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      55/60      3.41G     0.3365     0.5241     0.4184     0.8871         11        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:04<00:00,  6.08it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  2.30it/s]\n",
      "                   all         66         91      0.794      0.648      0.625      0.549      0.794      0.648      0.624      0.519\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      56/60      3.41G     0.3544     0.5223     0.4165     0.9088         11        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:03<00:00,  6.31it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  2.27it/s]\n",
      "                   all         66         91      0.749      0.659      0.622      0.549      0.841      0.615      0.627      0.516\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      57/60      3.41G     0.3415     0.5086     0.3991     0.8896         11        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:03<00:00,  6.36it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  2.32it/s]\n",
      "                   all         66         91      0.757       0.67      0.622      0.531      0.757       0.67      0.621      0.514\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      58/60      3.41G     0.3213       0.52     0.3922     0.8888         12        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:03<00:00,  6.30it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  2.31it/s]\n",
      "                   all         66         91      0.767      0.648      0.619      0.529      0.781      0.659      0.627       0.52\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      59/60      3.41G     0.3166     0.4804     0.3658     0.8847         11        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:03<00:00,  6.38it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  2.31it/s]\n",
      "                   all         66         91      0.778      0.656      0.628      0.539      0.791      0.667      0.628      0.517\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      60/60      3.41G     0.3164     0.4822     0.3489     0.8895         11        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:03<00:00,  6.34it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:02<00:00,  1.49it/s]\n",
      "                   all         66         91      0.776      0.648      0.627      0.536       0.79      0.659      0.626      0.516\n",
      "\n",
      "60 epochs completed in 0.129 hours.\n",
      "/home/jupyter-yixuan/.local/lib/python3.10/site-packages/ultralytics/yolo/utils/torch_utils.py:332: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  x = torch.load(f, map_location=torch.device('cpu'))\n",
      "Optimizer stripped from runs/segment/train9/weights/last.pt, 6.8MB\n",
      "Optimizer stripped from runs/segment/train9/weights/best.pt, 6.8MB\n",
      "\n",
      "Validating runs/segment/train9/weights/best.pt...\n",
      "Ultralytics YOLOv8.0.58 ðŸš€ Python-3.10.10 torch-2.4.0+cu121 CUDA:0 (Tesla V100-PCIE-16GB, 16151MiB)\n",
      "/home/jupyter-yixuan/.local/lib/python3.10/site-packages/ultralytics/nn/tasks.py:336: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(file, map_location='cpu'), file  # load\n",
      "YOLOv8n-seg summary (fused): 195 layers, 3258259 parameters, 0 gradients, 12.0 GFLOPs\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:02<00:00,  1.49it/s]\n",
      "                   all         66         91      0.758      0.637      0.649      0.564      0.788      0.626      0.655      0.524\n",
      "Speed: 0.2ms preprocess, 1.5ms inference, 0.0ms loss, 1.4ms postprocess per image\n",
      "Results saved to \u001b[1mruns/segment/train9\u001b[0m\n",
      "/home/jupyter-yixuan/.local/lib/python3.10/site-packages/ultralytics/nn/tasks.py:336: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(file, map_location='cpu'), file  # load\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# Load a model\n",
    "model = YOLO(\"yolov8n-seg.pt\")  # load an official model\n",
    "\n",
    "# Train the model\n",
    "results = model.train(data=\"/home/jupyter-yixuan/project_machine_learning/config.yaml\", epochs=60, imgsz=640)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7404c7aa-0cd9-4d46-9e36-d3267279180b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 448x640 1 Bear, 4.8ms\n",
      "Speed: 0.3ms preprocess, 4.8ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    }
   ],
   "source": [
    "### Predict Masks\n",
    "from ultralytics import YOLO\n",
    "import numpy as np\n",
    "\n",
    "import cv2\n",
    "\n",
    "model_path = '/home/jupyter-yixuan/project_machine_learning/runs/segment/train9/weights/last.pt'\n",
    "\n",
    "image_path = '/home/jupyter-yixuan/project_machine_learning/angry_bears/images/1.jpeg'\n",
    "\n",
    "img = cv2.imread(image_path)\n",
    "H, W, _ = img.shape\n",
    "\n",
    "model = YOLO(model_path)\n",
    "\n",
    "results = model(img)\n",
    "\n",
    "for result in results:\n",
    "    for j, mask in enumerate(result.masks.data):\n",
    "\n",
    "        mask = mask.cpu().numpy() * 255\n",
    "\n",
    "        mask = cv2.resize(mask, (W, H))\n",
    "\n",
    "        cv2.imwrite('/home/jupyter-yixuan/project_machine_learning/angry_bears/masks/1.jpeg', mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1b6e1e0f-42da-4d7e-b636-489dd672d009",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-yixuan/.local/lib/python3.10/site-packages/ultralytics/nn/tasks.py:336: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(file, map_location='cpu'), file  # load\n",
      "\n",
      "0: 448x640 1 Bear, 5.5ms\n",
      "Speed: 0.2ms preprocess, 5.5ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detection 1: Confidence = 0.9111\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "model_path = '/home/jupyter-yixuan/project_machine_learning/runs/segment/train9/weights/last.pt'\n",
    "image_path = '/home/jupyter-yixuan/project_machine_learning/angry_bears/images/1.jpeg'\n",
    "\n",
    "img = cv2.imread(image_path)\n",
    "H, W, _ = img.shape\n",
    "\n",
    "# Load the YOLO model\n",
    "model = YOLO(model_path)\n",
    "\n",
    "# Perform inference to get the masks\n",
    "results = model(img)\n",
    "\n",
    "# Iterate over each result (each detected object)\n",
    "for result in results:\n",
    "    # Iterate over each detected object in the result\n",
    "    for j, (mask, box) in enumerate(zip(result.masks.data, result.boxes)):\n",
    "        # Get the confidence score for this detection\n",
    "        confidence = box.conf.item()  # Convert tensor to a Python float\n",
    "        \n",
    "        # Print the confidence score\n",
    "        print(f\"Detection {j+1}: Confidence = {confidence:.4f}\")\n",
    "        \n",
    "        # Convert the mask to a numpy array and scale to 255\n",
    "        mask = mask.cpu().numpy() * 255\n",
    "        \n",
    "        # Resize the mask to match the original image dimensions\n",
    "        mask = cv2.resize(mask, (W, H))\n",
    "        \n",
    "        # Save the mask as an image\n",
    "        cv2.imwrite(f'/home/jupyter-yixuan/project_machine_learning/angry_bears/masks/mask_{j+1}.jpeg', mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12fdd649-f07f-4498-94e3-81a1768beff3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-yixuan/.local/lib/python3.10/site-packages/ultralytics/nn/tasks.py:336: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(file, map_location='cpu'), file  # load\n",
      "2024-08-12 21:41:57.024025: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-08-12 21:41:57.024063: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-08-12 21:41:57.024901: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-08-12 21:41:57.029984: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-08-12 21:41:57.762815: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "\n",
      "0: 448x640 1 Bear, 21.8ms\n",
      "Speed: 1.8ms preprocess, 21.8ms inference, 270.4ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "model_path = '/home/jupyter-yixuan/project_machine_learning/runs/segment/train9/weights/last.pt'\n",
    "image_path = '/home/jupyter-yixuan/project_machine_learning/angry_bears/images/1.jpeg'\n",
    "\n",
    "# Load the original image\n",
    "img = cv2.imread(image_path)\n",
    "H, W, _ = img.shape\n",
    "\n",
    "# Load the YOLO model\n",
    "model = YOLO(model_path)\n",
    "\n",
    "# Perform inference to get the masks\n",
    "results = model(img)\n",
    "\n",
    "# Initialize a blank canvas to store the filtered result\n",
    "filtered_img = np.zeros_like(img)\n",
    "\n",
    "for result in results:\n",
    "    for j, mask in enumerate(result.masks.data):\n",
    "        # Convert the mask to a numpy array and scale to 255\n",
    "        mask = mask.cpu().numpy() * 255\n",
    "        \n",
    "        # Resize the mask to match the original image dimensions\n",
    "        mask = cv2.resize(mask, (W, H))\n",
    "        \n",
    "        # Ensure the mask is binary (0 or 255)\n",
    "        _, binary_mask = cv2.threshold(mask, 128, 255, cv2.THRESH_BINARY)\n",
    "        \n",
    "        # Apply the mask to the original image using bitwise operations\n",
    "        masked_img = cv2.bitwise_and(img, img, mask=binary_mask.astype(np.uint8))\n",
    "        \n",
    "        # Combine with the filtered image\n",
    "        filtered_img = cv2.add(filtered_img, masked_img)\n",
    "\n",
    "# Save the filtered image\n",
    "cv2.imwrite('/home/jupyter-yixuan/project_machine_learning/angry_bears/masks/filtered_1.jpeg', filtered_img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2148d70f-e80f-4327-9c1b-7093264a7a55",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ultralytics.yolo.engine.results.Results object with attributes:\n",
       " \n",
       " _keys: ('boxes', 'masks', 'probs')\n",
       " boxes: ultralytics.yolo.engine.results.Boxes object\n",
       " keys: ['boxes', 'masks']\n",
       " masks: ultralytics.yolo.engine.results.Masks object\n",
       " names: {0: 'Bear'}\n",
       " orig_img: array([[[108, 101, 104],\n",
       "         [121, 116, 118],\n",
       "         [131, 126, 128],\n",
       "         ...,\n",
       "         [ 71,  72,  68],\n",
       "         [ 75,  79,  73],\n",
       "         [ 75,  79,  73]],\n",
       " \n",
       "        [[107, 100, 103],\n",
       "         [122, 117, 119],\n",
       "         [132, 127, 129],\n",
       "         ...,\n",
       "         [ 75,  76,  72],\n",
       "         [ 72,  76,  70],\n",
       "         [ 72,  76,  70]],\n",
       " \n",
       "        [[107, 100, 103],\n",
       "         [122, 117, 119],\n",
       "         [135, 130, 132],\n",
       "         ...,\n",
       "         [ 82,  83,  79],\n",
       "         [ 84,  88,  82],\n",
       "         [ 84,  88,  82]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[101,  99,  99],\n",
       "         [ 81,  79,  79],\n",
       "         [ 53,  51,  51],\n",
       "         ...,\n",
       "         [ 47,  41,  42],\n",
       "         [ 48,  43,  42],\n",
       "         [ 43,  38,  37]],\n",
       " \n",
       "        [[103, 101, 101],\n",
       "         [ 87,  85,  85],\n",
       "         [ 60,  58,  58],\n",
       "         ...,\n",
       "         [ 39,  33,  34],\n",
       "         [ 52,  47,  46],\n",
       "         [ 44,  39,  38]],\n",
       " \n",
       "        [[ 71,  69,  69],\n",
       "         [ 66,  64,  64],\n",
       "         [ 52,  50,  50],\n",
       "         ...,\n",
       "         [ 32,  26,  27],\n",
       "         [ 61,  56,  55],\n",
       "         [ 52,  47,  46]]], dtype=uint8)\n",
       " orig_shape: (184, 274)\n",
       " path: 'image0.jpg'\n",
       " probs: None\n",
       " speed: {'preprocess': 1.773834228515625, 'inference': 21.79551124572754, 'postprocess': 270.40982246398926}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27799139-4c1c-4852-b86b-fa4bfb3e32bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87946238-75b7-42cf-98fe-4044cae16a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.listdir('/home/jupyter-yixuan/project_machine_learning/dataset3/images/val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ffa8f49d-02b4-404c-bb86-6f1c3a585e56",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/val/009ebd2e28a2f9da.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/val/0f0e5a8f28537d2c.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/val/2cf16a85ea29a540.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/val/cbdf311b4ed017be.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/val/753feed7a87204eb.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/val/cb48bf7102604c06.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/val/1f3ecefff4c5fbb6.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/val/52db01e021d6c70a.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/val/919330c6aaba1902.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/val/d0de86a328d08631.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/val/40d317d2b084cb70.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/val/ae86abe83f830831.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/val/6bfe9d26db84585a.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/val/456ba2fbfc81d963.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/val/040ecb505679fde5.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/val/9629c5ca2f1501c2.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/val/6c413bd13881e4ea.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/val/cffa088717047957.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/val/c65ae1b4dfb5daed.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/val/c7d86272aa32b063.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/val/7442e4734f3418ce.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/val/c95acf66771a868e.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/val/981ff8cfd891a39a.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/val/2ca16e943e12343d.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/val/bcd925c7811ff07b.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/val/a043c1c94c493735.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/val/4f9ff9547d4bfddf.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/val/40deeb78aaef5f7e.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/val/98be41a21f007c23.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/val/2063cc4372428468.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/val/e926e910d9c55726.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/val/3a04c0b8966aea54.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/val/c03d0179ce3c983c.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/val/cb436ce30220421a.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/val/ae82153ffb586e28.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/val/1913a0ece8270f71.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/val/6fa67d4ab8b84fe9.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/val/c7b126470663c077.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/val/e3aabd8d29cea4f2.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/val/cec704931ea80bdd.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/val/cbcd47e43104de25.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/val/e1d6734a05335717.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/val/f30cedac9b8aed01.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/val/5f1fe36f063a89ff.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/val/13e6c2ad1cbc96a5.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/val/5257a813103f7b2a.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/val/c6aaf3b006df067a.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/val/8df9e6580dc9c3bb.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/val/dfe15d88d289f17b.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/val/41e07bd3461214b4.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/val/d6c71e577138b29b.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/val/5badea01bcd540f1.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/val/3874b3e6ab260f5b.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/val/de912daa2cb45a34.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/val/037ce70dca241ef6.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/val/b2fdd80d3b952823.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/val/b97495c5cb66d641.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/val/3315077e0d02efe5.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/val/d6a77ec80ff9fffa.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/val/0bdada492e85aa0e.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/val/385fea752fdf81bd.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/val/9592daa305158072.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/val/23a6e02006eefe09.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/val/9ae0c85363987e2f.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/val/562eb156f76596ba.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/val/3af4d90b2bae81d1.jpg\n"
     ]
    }
   ],
   "source": [
    "for file in os.listdir('/home/jupyter-yixuan/project_machine_learning/dataset3/images/val'):\n",
    "    print(os.path.join(original_folder, file ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6aeaf1ea-aefe-4e6b-8ea8-65ffed49255a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'456ba2fbfc81d963.jpg'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"/home/jupyter-yixuan/project_machine_learning/dataset3/images/val/456ba2fbfc81d963.jpg\".split('/')[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e10a3ce-5a4a-413e-881a-47fdf3b75321",
   "metadata": {},
   "source": [
    "## 2 Get masks for all training data and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "797722f4-7edd-4bfe-8596-064a2da717f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# original_folder = '/home/jupyter-yixuan/project_machine_learning/dataset3/images/train'\n",
    "original_folder = '/home/jupyter-yixuan/project_machine_learning/dataset3/images/val'\n",
    "# destination_folder = '/home/jupyter-yixuan/project_machine_learning/dataset3_filtered/train_masks'\n",
    "destination_folder = '/home/jupyter-yixuan/project_machine_learning/dataset3_filtered/val_masks'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3d3e5bf2-806f-4d44-bff9-a0d83c92d1da",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 448x640 1 Bear, 4.7ms\n",
      "Speed: 0.2ms preprocess, 4.7ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 3 Bears, 5.3ms\n",
      "Speed: 0.3ms preprocess, 5.3ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 2 Bears, 4.9ms\n",
      "Speed: 0.3ms preprocess, 4.9ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Bear, 7.0ms\n",
      "Speed: 0.3ms preprocess, 7.0ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Bear, 6.6ms\n",
      "Speed: 0.6ms preprocess, 6.6ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Bear, 4.8ms\n",
      "Speed: 0.3ms preprocess, 4.8ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 5.4ms\n",
      "Speed: 0.3ms preprocess, 5.4ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Bear, 5.4ms\n",
      "Speed: 0.3ms preprocess, 5.4ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/val/009ebd2e28a2f9da.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/val/0f0e5a8f28537d2c.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/val/2cf16a85ea29a540.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/val/cbdf311b4ed017be.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/val/753feed7a87204eb.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/val/cb48bf7102604c06.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/val/1f3ecefff4c5fbb6.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/val/52db01e021d6c70a.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 Bear, 5.3ms\n",
      "Speed: 0.3ms preprocess, 5.3ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 512x640 1 Bear, 7.9ms\n",
      "Speed: 0.3ms preprocess, 7.9ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 5.8ms\n",
      "Speed: 0.3ms preprocess, 5.8ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 Bear, 5.0ms\n",
      "Speed: 0.3ms preprocess, 5.0ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 5.4ms\n",
      "Speed: 0.3ms preprocess, 5.4ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Bear, 5.3ms\n",
      "Speed: 0.3ms preprocess, 5.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Bear, 4.9ms\n",
      "Speed: 0.3ms preprocess, 4.9ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 5.3ms\n",
      "Speed: 0.3ms preprocess, 5.3ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 1 Bear, 5.4ms\n",
      "Speed: 0.2ms preprocess, 5.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 5.9ms\n",
      "Speed: 0.4ms preprocess, 5.9ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 6.6ms\n",
      "Speed: 0.3ms preprocess, 6.6ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/val/919330c6aaba1902.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/val/d0de86a328d08631.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/val/40d317d2b084cb70.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/val/ae86abe83f830831.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/val/6bfe9d26db84585a.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/val/456ba2fbfc81d963.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/val/040ecb505679fde5.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/val/9629c5ca2f1501c2.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/val/6c413bd13881e4ea.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/val/cffa088717047957.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/val/c65ae1b4dfb5daed.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/val/c7d86272aa32b063.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 1 Bear, 6.1ms\n",
      "Speed: 0.4ms preprocess, 6.1ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 5.7ms\n",
      "Speed: 0.3ms preprocess, 5.7ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x480 1 Bear, 5.3ms\n",
      "Speed: 0.3ms preprocess, 5.3ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x448 2 Bears, 5.4ms\n",
      "Speed: 0.3ms preprocess, 5.4ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Bear, 5.4ms\n",
      "Speed: 0.3ms preprocess, 5.4ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 2 Bears, 4.8ms\n",
      "Speed: 0.3ms preprocess, 4.8ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Bear, 4.9ms\n",
      "Speed: 0.3ms preprocess, 4.9ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 416x640 2 Bears, 5.3ms\n",
      "Speed: 0.2ms preprocess, 5.3ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 (no detections), 5.2ms\n",
      "Speed: 0.3ms preprocess, 5.2ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/val/7442e4734f3418ce.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/val/c95acf66771a868e.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/val/981ff8cfd891a39a.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/val/2ca16e943e12343d.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/val/bcd925c7811ff07b.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/val/a043c1c94c493735.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/val/4f9ff9547d4bfddf.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/val/40deeb78aaef5f7e.jpg\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'data'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 27\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m results \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m results:\n\u001b[0;32m---> 27\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m j, mask \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmasks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m):\n\u001b[1;32m     28\u001b[0m             \u001b[38;5;66;03m# Convert the mask to a numpy array and scale to 255\u001b[39;00m\n\u001b[1;32m     29\u001b[0m             mask \u001b[38;5;241m=\u001b[39m mask\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy() \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m255\u001b[39m\n\u001b[1;32m     31\u001b[0m             \u001b[38;5;66;03m# Resize the mask to match the original image dimensions\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'data'"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "model_path = '/home/jupyter-yixuan/project_machine_learning/runs/segment/train9/weights/last.pt'\n",
    "\n",
    "# Load the YOLO model\n",
    "model = YOLO(model_path)\n",
    "\n",
    "# image_path = '/home/jupyter-yixuan/project_machine_learning/angry_bears/images/1.jpeg'\n",
    "for image_path in os.listdir(original_folder):\n",
    "    extension = image_path.split('.')[-1]\n",
    "    if extension == 'jpg':    \n",
    "        image_path = os.path.join(original_folder, image_path)\n",
    "        # Load the original image\n",
    "        # print(image_path)\n",
    "        img = cv2.imread(image_path)\n",
    "        H, W, _ = img.shape\n",
    "        \n",
    "        # Perform inference to get the masks\n",
    "        results = model(img)\n",
    "    \n",
    "        if results is not None:\n",
    "            \n",
    "            for result in results:\n",
    "                for j, mask in enumerate(result.masks.data):\n",
    "                    # Convert the mask to a numpy array and scale to 255\n",
    "                    mask = mask.cpu().numpy() * 255\n",
    "                    \n",
    "                    # Resize the mask to match the original image dimensions\n",
    "                    mask = cv2.resize(mask, (W, H))\n",
    "                    \n",
    "                    # # Ensure the mask is binary (0 or 255)\n",
    "                    # _, binary_mask = cv2.threshold(mask, 128, 255, cv2.THRESH_BINARY)\n",
    "                    \n",
    "                    # # Apply the mask to the original image using bitwise operations\n",
    "                    # masked_img = cv2.bitwise_and(img, img, mask=binary_mask.astype(np.uint8))\n",
    "                    \n",
    "                    # # Combine with the filtered image\n",
    "                    # filtered_img = cv2.add(filtered_img, masked_img)\n",
    "            \n",
    "            # Save the filtered image\n",
    "            new_path = os.path.join(destination_folder, image_path.split('/')[-1])\n",
    "            cv2.imwrite(new_path, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cd27b777-9116-4f1d-a331-03353f651ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_folder = '/home/jupyter-yixuan/project_machine_learning/dataset3/images/val'\n",
    "destination_folder = '/home/jupyter-yixuan/project_machine_learning/dataset3_filtered/val_masks'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4b5472e8-df09-412d-b7c2-7ebf4060e88e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 448x640 1 Bear, 4.8ms\n",
      "Speed: 0.2ms preprocess, 4.8ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 3 Bears, 5.2ms\n",
      "Speed: 0.3ms preprocess, 5.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 2 Bears, 5.1ms\n",
      "Speed: 0.3ms preprocess, 5.1ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Bear, 5.6ms\n",
      "Speed: 0.3ms preprocess, 5.6ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Bear, 5.1ms\n",
      "Speed: 0.3ms preprocess, 5.1ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Bear, 4.9ms\n",
      "Speed: 0.3ms preprocess, 4.9ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 5.1ms\n",
      "Speed: 0.3ms preprocess, 5.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Bear, 5.3ms\n",
      "Speed: 0.3ms preprocess, 5.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Bear, 4.8ms\n",
      "Speed: 0.3ms preprocess, 4.8ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 512x640 1 Bear, 5.1ms\n",
      "Speed: 0.3ms preprocess, 5.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 5.3ms\n",
      "Speed: 0.3ms preprocess, 5.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 Bear, 5.5ms\n",
      "Speed: 0.3ms preprocess, 5.5ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 5.1ms\n",
      "Speed: 0.3ms preprocess, 5.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Bear, 5.3ms\n",
      "Speed: 0.3ms preprocess, 5.3ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Bear, 4.9ms\n",
      "Speed: 0.3ms preprocess, 4.9ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 5.1ms\n",
      "Speed: 0.3ms preprocess, 5.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 1 Bear, 5.8ms\n",
      "Speed: 0.3ms preprocess, 5.8ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 5.2ms\n",
      "Speed: 0.3ms preprocess, 5.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 5.0ms\n",
      "Speed: 0.3ms preprocess, 5.0ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Bear, 5.3ms\n",
      "Speed: 0.3ms preprocess, 5.3ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 5.3ms\n",
      "Speed: 0.3ms preprocess, 5.3ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x480 1 Bear, 5.3ms\n",
      "Speed: 0.3ms preprocess, 5.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x448 2 Bears, 5.3ms\n",
      "Speed: 0.3ms preprocess, 5.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Bear, 5.3ms\n",
      "Speed: 0.3ms preprocess, 5.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 2 Bears, 5.0ms\n",
      "Speed: 0.3ms preprocess, 5.0ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Bear, 5.0ms\n",
      "Speed: 0.3ms preprocess, 5.0ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 416x640 2 Bears, 7.8ms\n",
      "Speed: 0.3ms preprocess, 7.8ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 (no detections), 5.4ms\n",
      "Speed: 0.3ms preprocess, 5.4ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 5.0ms\n",
      "Speed: 0.3ms preprocess, 5.0ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 2 Bears, 7.1ms\n",
      "Speed: 0.3ms preprocess, 7.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 5.9ms\n",
      "Speed: 0.3ms preprocess, 5.9ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Bear, 5.2ms\n",
      "Speed: 0.3ms preprocess, 5.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Bear, 4.8ms\n",
      "Speed: 0.3ms preprocess, 4.8ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 5.3ms\n",
      "Speed: 0.3ms preprocess, 5.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 2 Bears, 5.2ms\n",
      "Speed: 0.3ms preprocess, 5.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 5.3ms\n",
      "Speed: 0.3ms preprocess, 5.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Bear, 5.2ms\n",
      "Speed: 0.3ms preprocess, 5.2ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x448 1 Bear, 6.4ms\n",
      "Speed: 0.5ms preprocess, 6.4ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 2 Bears, 5.3ms\n",
      "Speed: 0.3ms preprocess, 5.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 5.3ms\n",
      "Speed: 0.3ms preprocess, 5.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 4.9ms\n",
      "Speed: 0.3ms preprocess, 4.9ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Bear, 5.6ms\n",
      "Speed: 0.4ms preprocess, 5.6ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 2 Bears, 7.1ms\n",
      "Speed: 0.3ms preprocess, 7.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 4.7ms\n",
      "Speed: 0.3ms preprocess, 4.7ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 512x640 1 Bear, 5.1ms\n",
      "Speed: 0.3ms preprocess, 5.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 8.6ms\n",
      "Speed: 0.3ms preprocess, 8.6ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Bear, 5.3ms\n",
      "Speed: 0.3ms preprocess, 5.3ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 5.0ms\n",
      "Speed: 0.3ms preprocess, 5.0ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Bear, 5.5ms\n",
      "Speed: 0.3ms preprocess, 5.5ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 576x640 1 Bear, 5.5ms\n",
      "Speed: 0.3ms preprocess, 5.5ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Bear, 5.3ms\n",
      "Speed: 0.3ms preprocess, 5.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 5.4ms\n",
      "Speed: 0.3ms preprocess, 5.4ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 4.8ms\n",
      "Speed: 0.3ms preprocess, 4.8ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Bear, 5.2ms\n",
      "Speed: 0.3ms preprocess, 5.2ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 1 Bear, 5.4ms\n",
      "Speed: 0.3ms preprocess, 5.4ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Bear, 5.3ms\n",
      "Speed: 0.3ms preprocess, 5.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 5.4ms\n",
      "Speed: 0.3ms preprocess, 5.4ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 512x640 1 Bear, 5.0ms\n",
      "Speed: 0.3ms preprocess, 5.0ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 5.2ms\n",
      "Speed: 0.3ms preprocess, 5.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x448 1 Bear, 5.2ms\n",
      "Speed: 0.3ms preprocess, 5.2ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 608x640 1 Bear, 21.6ms\n",
      "Speed: 0.3ms preprocess, 21.6ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Bear, 5.3ms\n",
      "Speed: 0.4ms preprocess, 5.3ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x480 2 Bears, 5.2ms\n",
      "Speed: 0.3ms preprocess, 5.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 416x640 2 Bears, 5.3ms\n",
      "Speed: 0.2ms preprocess, 5.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Bear, 5.4ms\n",
      "Speed: 0.3ms preprocess, 5.4ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 5.1ms\n",
      "Speed: 0.3ms preprocess, 5.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "model_path = '/home/jupyter-yixuan/project_machine_learning/runs/segment/train9/weights/last.pt'\n",
    "\n",
    "# Load the YOLO model\n",
    "model = YOLO(model_path)\n",
    "\n",
    "# image_path = '/home/jupyter-yixuan/project_machine_learning/angry_bears/images/1.jpeg'\n",
    "for image_path in os.listdir(original_folder):\n",
    "    extension = image_path.split('.')[-1]\n",
    "    if extension == 'jpg':    \n",
    "        image_path = os.path.join(original_folder, image_path)\n",
    "        # Load the original image\n",
    "        # print(image_path)\n",
    "        img = cv2.imread(image_path)\n",
    "        if img is not None:\n",
    "            H, W, _ = img.shape\n",
    "            \n",
    "            # Perform inference to get the masks\n",
    "            results = model(img)\n",
    "        \n",
    "\n",
    "                \n",
    "            for result in results:\n",
    "                if result.masks is not None:\n",
    "                    for j, mask in enumerate(result.masks.data):\n",
    "                        # Convert the mask to a numpy array and scale to 255\n",
    "                        mask = mask.cpu().numpy() * 255\n",
    "                        \n",
    "                        # Resize the mask to match the original image dimensions\n",
    "                        mask = cv2.resize(mask, (W, H))\n",
    "                        \n",
    "                        # # Ensure the mask is binary (0 or 255)\n",
    "                        # _, binary_mask = cv2.threshold(mask, 128, 255, cv2.THRESH_BINARY)\n",
    "                        \n",
    "                        # # Apply the mask to the original image using bitwise operations\n",
    "                        # masked_img = cv2.bitwise_and(img, img, mask=binary_mask.astype(np.uint8))\n",
    "                        \n",
    "                        # # Combine with the filtered image\n",
    "                        # filtered_img = cv2.add(filtered_img, masked_img)\n",
    "                \n",
    "                # Save the filtered image\n",
    "            new_path = os.path.join(destination_folder, image_path.split('/')[-1])\n",
    "            cv2.imwrite(new_path, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f4795f-d806-4e35-8bd7-363134b19c06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "da77e83b-4011-4075-97cb-8ff5160a7f24",
   "metadata": {},
   "source": [
    "## 3 Mask original images and remove background for all training and validation data in dataset3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e60645f4-8128-4631-b0c7-28828afd2f06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-12 22:50:11.286378: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-08-12 22:50:11.286412: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-08-12 22:50:11.287083: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-08-12 22:50:11.290842: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-08-12 22:50:12.048106: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "\n",
      "0: 448x640 1 Bear, 23.7ms\n",
      "Speed: 1.8ms preprocess, 23.7ms inference, 250.2ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the code for one image\n",
    "\n",
    "from ultralytics import YOLO\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "model_path = '/home/jupyter-yixuan/project_machine_learning/runs/segment/train9/weights/last.pt'\n",
    "image_path = '/home/jupyter-yixuan/project_machine_learning/dataset3/images/val/1f3ecefff4c5fbb6.jpg'\n",
    "\n",
    "# Load the original image\n",
    "img = cv2.imread(image_path)\n",
    "H, W, _ = img.shape\n",
    "\n",
    "# Load the YOLO model\n",
    "model = YOLO(model_path)\n",
    "\n",
    "# Perform inference to get the masks\n",
    "results = model(img)\n",
    "\n",
    "# Initialize a blank canvas to store the filtered result\n",
    "filtered_img = np.zeros_like(img)\n",
    "\n",
    "for result in results:\n",
    "    for j, mask in enumerate(result.masks.data):\n",
    "        # Convert the mask to a numpy array and scale to 255\n",
    "        mask = mask.cpu().numpy() * 255\n",
    "        \n",
    "        # Resize the mask to match the original image dimensions\n",
    "        mask = cv2.resize(mask, (W, H))\n",
    "        \n",
    "        # Ensure the mask is binary (0 or 255)\n",
    "        _, binary_mask = cv2.threshold(mask, 128, 255, cv2.THRESH_BINARY)\n",
    "        \n",
    "        # Apply the mask to the original image using bitwise operations\n",
    "        masked_img = cv2.bitwise_and(img, img, mask=binary_mask.astype(np.uint8))\n",
    "        \n",
    "        # Combine with the filtered image\n",
    "        filtered_img = cv2.add(filtered_img, masked_img)\n",
    "\n",
    "# Save the filtered image\n",
    "cv2.imwrite('/home/jupyter-yixuan/project_machine_learning/angry_bears/masks/filtered_2.jpeg', filtered_img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "285cc6dd-cdb8-4f68-b6b6-f9967223c3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_folder = '/home/jupyter-yixuan/project_machine_learning/dataset3/images/train'\n",
    "destination_folder = '/home/jupyter-yixuan/project_machine_learning/dataset3_filtered/train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3a73bd40-f83b-45b6-8073-a2280f433681",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-yixuan/.local/lib/python3.10/site-packages/ultralytics/nn/tasks.py:336: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(file, map_location='cpu'), file  # load\n",
      "\n",
      "0: 448x640 1 Bear, 4.9ms\n",
      "Speed: 0.3ms preprocess, 4.9ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 6.1ms\n",
      "Speed: 0.4ms preprocess, 6.1ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 1 Bear, 5.7ms\n",
      "Speed: 0.4ms preprocess, 5.7ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 5.4ms\n",
      "Speed: 0.3ms preprocess, 5.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Bear, 5.4ms\n",
      "Speed: 0.4ms preprocess, 5.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 Bear, 5.1ms\n",
      "Speed: 0.4ms preprocess, 5.1ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/fb87f76487592286.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/5b024aa42fe31ec2.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/a1e673fa305a9eaf.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/25e9645b7b397895.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/14af07a7b169eb9b.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/6e9019a2167ab957.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x608 1 Bear, 22.0ms\n",
      "Speed: 0.4ms preprocess, 22.0ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Bear, 5.4ms\n",
      "Speed: 0.4ms preprocess, 5.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x544 1 Bear, 23.4ms\n",
      "Speed: 0.4ms preprocess, 23.4ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 5.5ms\n",
      "Speed: 0.4ms preprocess, 5.5ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 5.1ms\n",
      "Speed: 0.3ms preprocess, 5.1ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Bear, 5.6ms\n",
      "Speed: 0.4ms preprocess, 5.6ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 5.2ms\n",
      "Speed: 0.4ms preprocess, 5.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Bear, 5.5ms\n",
      "Speed: 0.4ms preprocess, 5.5ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/f5a9cb58ef9c0fe4.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/698da55dc946e318.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/cca35bb83ae0b70a.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/07ab0312b0cfdcf3.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/b4fbdd8f703bb92a.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/211027fb4ce253c0.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/41a590d831cc6949.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/2dcf884d0d008018.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 416x640 3 Bears, 5.4ms\n",
      "Speed: 0.3ms preprocess, 5.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Bear, 5.3ms\n",
      "Speed: 0.4ms preprocess, 5.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 5.4ms\n",
      "Speed: 0.4ms preprocess, 5.4ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 5.0ms\n",
      "Speed: 0.4ms preprocess, 5.0ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x512 1 Bear, 22.2ms\n",
      "Speed: 0.4ms preprocess, 22.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Bear, 5.4ms\n",
      "Speed: 0.4ms preprocess, 5.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 5.1ms\n",
      "Speed: 0.3ms preprocess, 5.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 5.2ms\n",
      "Speed: 0.4ms preprocess, 5.2ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 5.0ms\n",
      "Speed: 0.4ms preprocess, 5.0ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Bear, 5.4ms\n",
      "Speed: 0.4ms preprocess, 5.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/d6bccf5b4379b1ef.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/44d122bec5830567.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/3259efe97c7d98a1.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/163af8ddb0c94822.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/09b72f4966fbf1d1.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/347b479747d7e65f.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/8aeb6a0f712b3694.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/2b16490d59255588.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/f712e12da0daf9cd.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/a753de6b8755393f.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/1c226de4764addf7.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 448x640 1 Bear, 5.4ms\n",
      "Speed: 0.3ms preprocess, 5.4ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 1 Bear, 5.4ms\n",
      "Speed: 0.3ms preprocess, 5.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 5.3ms\n",
      "Speed: 0.3ms preprocess, 5.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Bear, 6.1ms\n",
      "Speed: 0.5ms preprocess, 6.1ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x448 1 Bear, 5.1ms\n",
      "Speed: 0.4ms preprocess, 5.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 5.1ms\n",
      "Speed: 0.4ms preprocess, 5.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 5.3ms\n",
      "Speed: 0.4ms preprocess, 5.3ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 5.9ms\n",
      "Speed: 0.3ms preprocess, 5.9ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Bear, 5.3ms\n",
      "Speed: 0.4ms preprocess, 5.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 5.5ms\n",
      "Speed: 0.4ms preprocess, 5.5ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/359372a4cf35e2d0.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/444b0af9f30fb842.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/3b9ed278832a85d2.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/4676bb9e02b5f6e9.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/2f224b29e640a9e6.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/2dfbd2133ceb0742.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/1e936239c3e0f828.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/8575aecb96c78023.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/0848e999b4fa1a8c.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/06047b28fcb119e2.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 512x640 1 Bear, 5.3ms\n",
      "Speed: 0.4ms preprocess, 5.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 5.1ms\n",
      "Speed: 0.4ms preprocess, 5.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 5.1ms\n",
      "Speed: 0.4ms preprocess, 5.1ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 512x640 1 Bear, 5.5ms\n",
      "Speed: 0.4ms preprocess, 5.5ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Bear, 5.1ms\n",
      "Speed: 0.4ms preprocess, 5.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Bear, 5.3ms\n",
      "Speed: 0.4ms preprocess, 5.3ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Bear, 4.8ms\n",
      "Speed: 0.4ms preprocess, 4.8ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 5.4ms\n",
      "Speed: 0.4ms preprocess, 5.4ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 2 Bears, 5.1ms\n",
      "Speed: 0.4ms preprocess, 5.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 5.4ms\n",
      "Speed: 0.4ms preprocess, 5.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 416x640 1 Bear, 5.5ms\n",
      "Speed: 0.3ms preprocess, 5.5ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/268b623eed11d300.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/ad80fbba1bc95661.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/b7d9c07c34cf3555.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/c1beb25dbefba7a8.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/e10765064ca1e985.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/2a55f442f555f727.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/1112a2da04b5bfbb.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/175d6cf7e05d81b3.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/1d0eaf82c86419e2.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/2190c79e3b504ddf.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/2151b871209889a6.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 448x640 1 Bear, 5.5ms\n",
      "Speed: 0.3ms preprocess, 5.5ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 2 Bears, 5.5ms\n",
      "Speed: 0.4ms preprocess, 5.5ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Bear, 4.9ms\n",
      "Speed: 0.4ms preprocess, 4.9ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 5.3ms\n",
      "Speed: 0.4ms preprocess, 5.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 5.8ms\n",
      "Speed: 0.4ms preprocess, 5.8ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 1 Bear, 5.5ms\n",
      "Speed: 0.3ms preprocess, 5.5ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Bear, 5.4ms\n",
      "Speed: 0.4ms preprocess, 5.4ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x480 1 Bear, 5.5ms\n",
      "Speed: 0.3ms preprocess, 5.5ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x448 1 Bear, 5.1ms\n",
      "Speed: 0.4ms preprocess, 5.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 5.5ms\n",
      "Speed: 0.4ms preprocess, 5.5ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 512x640 1 Bear, 5.4ms\n",
      "Speed: 0.4ms preprocess, 5.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/9534992110560266.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/03fbbcd4708819c9.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/17279d76c1f56b03.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/06a2621ed24163a7.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/13a83f7bcf4541c4.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/4b9b2dc12f9aa582.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/8c9d0c019e694c8a.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/35913a5b3fcfa39d.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/1db4bd668dc050cf.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/00c3583647e8432a.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 448x640 1 Bear, 5.5ms\n",
      "Speed: 0.3ms preprocess, 5.5ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Bear, 5.4ms\n",
      "Speed: 0.4ms preprocess, 5.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 5.7ms\n",
      "Speed: 0.4ms preprocess, 5.7ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Bear, 5.5ms\n",
      "Speed: 0.4ms preprocess, 5.5ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 Bear, 7.6ms\n",
      "Speed: 0.4ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Bear, 5.3ms\n",
      "Speed: 0.4ms preprocess, 5.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 1 Bear, 5.5ms\n",
      "Speed: 0.4ms preprocess, 5.5ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x544 1 Bear, 5.1ms\n",
      "Speed: 0.4ms preprocess, 5.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 5.4ms\n",
      "Speed: 0.4ms preprocess, 5.4ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/5c95f23ce7b91595.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/0c5e03e7538516df.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/1c675c0841b1e3fd.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/140a5b5acb5d45ee.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/5367e1c7d739327c.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/0c41cdb9024316c3.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/988e58ac110d335b.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/93280de3549014c0.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/441a699ce9fb67b7.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/a662060233c4b6e8.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 1 Bear, 6.4ms\n",
      "Speed: 0.4ms preprocess, 6.4ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 512x640 2 Bears, 5.4ms\n",
      "Speed: 0.3ms preprocess, 5.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Bear, 5.2ms\n",
      "Speed: 0.4ms preprocess, 5.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Bear, 5.1ms\n",
      "Speed: 0.4ms preprocess, 5.1ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 5.4ms\n",
      "Speed: 0.4ms preprocess, 5.4ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 5.0ms\n",
      "Speed: 0.3ms preprocess, 5.0ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Bear, 5.3ms\n",
      "Speed: 0.4ms preprocess, 5.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 5.1ms\n",
      "Speed: 0.4ms preprocess, 5.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Bear, 5.7ms\n",
      "Speed: 0.4ms preprocess, 5.7ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x480 1 Bear, 5.6ms\n",
      "Speed: 0.3ms preprocess, 5.6ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/ff4b5ed2cb19a112.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/2e9474fa9277fb48.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/032530bd2dbf39b4.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/e4745a37c7ebb0db.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/5246c5f5423d1718.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/1a4d60e0b47e5af9.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/02cb270ce28df2a4.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/2a7f6eb4c8b52acc.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/06a5686c1d1c31e6.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/07eb470c179a0879.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 416x640 1 Bear, 5.1ms\n",
      "Speed: 0.3ms preprocess, 5.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 5.3ms\n",
      "Speed: 0.4ms preprocess, 5.3ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 512x640 1 Bear, 5.4ms\n",
      "Speed: 0.4ms preprocess, 5.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 512x640 1 Bear, 5.0ms\n",
      "Speed: 0.4ms preprocess, 5.0ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Bear, 6.8ms\n",
      "Speed: 0.5ms preprocess, 6.8ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 7.1ms\n",
      "Speed: 0.3ms preprocess, 7.1ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 5.0ms\n",
      "Speed: 0.4ms preprocess, 5.0ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Bear, 5.4ms\n",
      "Speed: 0.4ms preprocess, 5.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 5.4ms\n",
      "Speed: 0.4ms preprocess, 5.4ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Bear, 5.5ms\n",
      "Speed: 0.4ms preprocess, 5.5ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 5.6ms\n",
      "Speed: 0.4ms preprocess, 5.6ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Bear, 5.6ms\n",
      "Speed: 0.4ms preprocess, 5.6ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/0deeadeb07c457fd.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/83d11607b4e3352a.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/3f883da1c03648df.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/ef72b8511b655153.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/2da591d3fa2be186.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/0431ed40b990d9f2.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/ace8be454d4028fe.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/cd037adca6d2c298.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/129ca214b7d8f33d.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/2d79721d3c5222e9.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/122ead55b707d8ae.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/172719ee88c86c91.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 448x640 1 Bear, 5.4ms\n",
      "Speed: 0.4ms preprocess, 5.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 4.9ms\n",
      "Speed: 0.4ms preprocess, 4.9ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 512x640 1 Bear, 5.4ms\n",
      "Speed: 0.4ms preprocess, 5.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 5.6ms\n",
      "Speed: 0.3ms preprocess, 5.6ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 512x640 1 Bear, 5.5ms\n",
      "Speed: 0.3ms preprocess, 5.5ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Bear, 5.2ms\n",
      "Speed: 0.4ms preprocess, 5.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Bear, 4.8ms\n",
      "Speed: 0.3ms preprocess, 4.8ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 5.3ms\n",
      "Speed: 0.3ms preprocess, 5.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 4.7ms\n",
      "Speed: 0.4ms preprocess, 4.7ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 5.1ms\n",
      "Speed: 0.4ms preprocess, 5.1ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/0a97447a4d406c9a.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/10d8c12bd02c738e.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/8cd57f5785d08f89.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/0d3076545e3c46ab.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/3ed24d25b718c2f6.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/f27440c164443e8b.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/95d2aac60b581735.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/9768ca8db7e72e03.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/082e417c281961b9.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/71eb234697d1417f.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 576x640 1 Bear, 22.6ms\n",
      "Speed: 0.4ms preprocess, 22.6ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x448 1 Bear, 5.4ms\n",
      "Speed: 0.4ms preprocess, 5.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 5.7ms\n",
      "Speed: 0.4ms preprocess, 5.7ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Bear, 5.6ms\n",
      "Speed: 0.4ms preprocess, 5.6ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Bear, 5.0ms\n",
      "Speed: 0.4ms preprocess, 5.0ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Bear, 4.8ms\n",
      "Speed: 0.4ms preprocess, 4.8ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 1 Bear, 5.6ms\n",
      "Speed: 0.4ms preprocess, 5.6ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Bear, 5.4ms\n",
      "Speed: 0.4ms preprocess, 5.4ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Bear, 5.0ms\n",
      "Speed: 0.4ms preprocess, 5.0ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 5.5ms\n",
      "Speed: 0.4ms preprocess, 5.5ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 5.1ms\n",
      "Speed: 0.4ms preprocess, 5.1ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/4223beb61e16b132.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/627f1a1206ba5332.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/3035a1266564dbcf.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/aaa4b4200e0dbc6e.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/5e3b5b9c92f5f4a2.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/2207ebf60f64fad6.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/fe765f5a9e897470.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/087e2627fa9ca26d.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/6e72a4b0e781099a.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/4a881bef15a88dc0.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/4725edfd56effe07.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 640x480 1 Bear, 5.7ms\n",
      "Speed: 0.4ms preprocess, 5.7ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Bear, 5.5ms\n",
      "Speed: 0.4ms preprocess, 5.5ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 5.5ms\n",
      "Speed: 0.4ms preprocess, 5.5ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 5.0ms\n",
      "Speed: 0.4ms preprocess, 5.0ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 4.7ms\n",
      "Speed: 0.4ms preprocess, 4.7ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 512x640 1 Bear, 5.3ms\n",
      "Speed: 0.4ms preprocess, 5.3ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 5.6ms\n",
      "Speed: 0.4ms preprocess, 5.6ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 1 Bear, 5.4ms\n",
      "Speed: 0.4ms preprocess, 5.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 5.5ms\n",
      "Speed: 0.3ms preprocess, 5.5ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Bear, 5.1ms\n",
      "Speed: 0.4ms preprocess, 5.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 5.6ms\n",
      "Speed: 0.4ms preprocess, 5.6ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/0739ebfb4c929636.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/190a7be8118045e5.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/0459f82b4335ff0e.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/2774024f0e86e88a.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/9002ac1295852dbf.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/168d0228ba0d29e0.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/b0ec193201b277a2.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/123587cc1ab6af8f.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/93ed1f2964446856.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/3fe1fb5f385489e7.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/1ae6c3366d69ed44.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 Bears, 5.7ms\n",
      "Speed: 0.3ms preprocess, 5.7ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 5.6ms\n",
      "Speed: 0.3ms preprocess, 5.6ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 416x640 1 Bear, 5.4ms\n",
      "Speed: 0.4ms preprocess, 5.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 5.1ms\n",
      "Speed: 0.4ms preprocess, 5.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 5.1ms\n",
      "Speed: 0.3ms preprocess, 5.1ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Bear, 5.4ms\n",
      "Speed: 0.4ms preprocess, 5.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Bear, 5.0ms\n",
      "Speed: 0.3ms preprocess, 5.0ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x480 1 Bear, 5.5ms\n",
      "Speed: 0.4ms preprocess, 5.5ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 2 Bears, 5.4ms\n",
      "Speed: 0.4ms preprocess, 5.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 544x640 1 Bear, 22.1ms\n",
      "Speed: 0.4ms preprocess, 22.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Bear, 5.4ms\n",
      "Speed: 0.4ms preprocess, 5.4ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/2ff514fb35dd9acd.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/05e2a5f4f3f0f402.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/88b2a69722a573e4.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/13cf7e68c5f95ece.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/1c20eccd2ebb6934.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/6a04df70abe6457e.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/01d9867ddfb7c0ab.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/4d47d9d7b628c4ff.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/0d48f4979b8c511f.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/343f2e7b049715bf.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 Bear, 5.1ms\n",
      "Speed: 0.4ms preprocess, 5.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 5.5ms\n",
      "Speed: 0.4ms preprocess, 5.5ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x448 1 Bear, 5.3ms\n",
      "Speed: 0.4ms preprocess, 5.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 2 Bears, 5.6ms\n",
      "Speed: 0.3ms preprocess, 5.6ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Bear, 6.3ms\n",
      "Speed: 0.3ms preprocess, 6.3ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 3 Bears, 5.1ms\n",
      "Speed: 0.3ms preprocess, 5.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 5.0ms\n",
      "Speed: 0.4ms preprocess, 5.0ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 5.3ms\n",
      "Speed: 0.3ms preprocess, 5.3ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x480 1 Bear, 5.1ms\n",
      "Speed: 0.4ms preprocess, 5.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 5.5ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/07b74b8a3d95b09e.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/3b6975387626cbb1.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/3a804f5b5bdd9875.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/4b7dda4a42b50d3b.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/2aa5d405e37e6a5b.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/5749a9ff4de7c4b6.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/0d0009c9745a0cb1.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/59970327e9f5cfbd.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/24641386ddee3453.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/2f67b99513b47b93.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Speed: 0.4ms preprocess, 5.5ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 2 Bears, 5.1ms\n",
      "Speed: 0.4ms preprocess, 5.1ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 4.9ms\n",
      "Speed: 0.4ms preprocess, 4.9ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 4.7ms\n",
      "Speed: 0.4ms preprocess, 4.7ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Bear, 5.5ms\n",
      "Speed: 0.4ms preprocess, 5.5ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 5.2ms\n",
      "Speed: 0.3ms preprocess, 5.2ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/97058787c18e1fc5.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/90c68d33a454b222.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/0badd36382a6cb7c.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/4603e764e1886014.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/f3459d67092b0756.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 Bear, 5.1ms\n",
      "Speed: 0.4ms preprocess, 5.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Bear, 5.1ms\n",
      "Speed: 0.3ms preprocess, 5.1ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 544x640 1 Bear, 5.4ms\n",
      "Speed: 0.4ms preprocess, 5.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 5.4ms\n",
      "Speed: 0.4ms preprocess, 5.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 4.7ms\n",
      "Speed: 0.4ms preprocess, 4.7ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 5.1ms\n",
      "Speed: 0.4ms preprocess, 5.1ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 5.1ms\n",
      "Speed: 0.4ms preprocess, 5.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Bear, 6.1ms\n",
      "Speed: 0.3ms preprocess, 6.1ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x448 1 Bear, 6.9ms\n",
      "Speed: 0.3ms preprocess, 6.9ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 2 Bears, 5.4ms\n",
      "Speed: 0.3ms preprocess, 5.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/789567af5f39ba5b.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/33585bb9d6c006b2.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/0945ece2861553ce.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/617ad00acf01b18c.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/98be828d7d5e529b.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/da04333faa4842b8.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/1f9ce045dcbb6e95.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/051d51cf287e090b.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/ede197cef5624654.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/050fa018ad440cde.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/461a3fb2b4c67177.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 1 Bear, 5.4ms\n",
      "Speed: 0.4ms preprocess, 5.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 Bear, 5.4ms\n",
      "Speed: 0.4ms preprocess, 5.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 5.5ms\n",
      "Speed: 0.3ms preprocess, 5.5ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Bear, 5.5ms\n",
      "Speed: 0.4ms preprocess, 5.5ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Bear, 4.7ms\n",
      "Speed: 0.4ms preprocess, 4.7ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 1 Bear, 5.7ms\n",
      "Speed: 0.3ms preprocess, 5.7ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x544 1 Bear, 5.6ms\n",
      "Speed: 0.4ms preprocess, 5.6ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 5.3ms\n",
      "Speed: 0.3ms preprocess, 5.3ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Bear, 5.6ms\n",
      "Speed: 0.4ms preprocess, 5.6ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Bear, 5.2ms\n",
      "Speed: 0.4ms preprocess, 5.2ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Bear, 5.0ms\n",
      "Speed: 0.4ms preprocess, 5.0ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/27ce7def88084837.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/30b3846a183af1b7.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/354fbba7bf3f05cc.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/183ba5e0d1d44549.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/4dbd5ad40ab27aec.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/178dc55fe6897baa.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/093d7062fd614572.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/958a1366e03269db.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/de0955164d13884a.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/7213ca31862b0cba.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/1a0bc4938b19d993.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 1 Bear, 5.0ms\n",
      "Speed: 0.4ms preprocess, 5.0ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Bear, 5.2ms\n",
      "Speed: 0.4ms preprocess, 5.2ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 5.5ms\n",
      "Speed: 0.4ms preprocess, 5.5ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 2 Bears, 5.2ms\n",
      "Speed: 0.4ms preprocess, 5.2ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Bear, 5.5ms\n",
      "Speed: 0.4ms preprocess, 5.5ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 1 Bear, 5.6ms\n",
      "Speed: 0.3ms preprocess, 5.6ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 5.4ms\n",
      "Speed: 0.4ms preprocess, 5.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 5.5ms\n",
      "Speed: 0.4ms preprocess, 5.5ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 Bear, 5.4ms\n",
      "Speed: 0.4ms preprocess, 5.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 2 Bears, 5.6ms\n",
      "Speed: 0.4ms preprocess, 5.6ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Bear, 5.1ms\n",
      "Speed: 0.4ms preprocess, 5.1ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 5.5ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/f8d01701e5e78033.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/0c0349b4404fc466.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/59639446a3821207.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/27f993af1802c25b.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/924e12a29c8c3321.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/2eca6a8c65235ebe.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/93ad58b6890d44fa.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/2e8fb2f4f601cc14.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/3d73da05f9705ed3.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/e6c94b1fc0409191.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/6c59dd3c0175dd23.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Speed: 0.3ms preprocess, 5.5ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Bear, 5.7ms\n",
      "Speed: 0.4ms preprocess, 5.7ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Bear, 4.7ms\n",
      "Speed: 0.4ms preprocess, 4.7ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 5.5ms\n",
      "Speed: 0.3ms preprocess, 5.5ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 5.0ms\n",
      "Speed: 0.3ms preprocess, 5.0ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Bear, 5.3ms\n",
      "Speed: 0.3ms preprocess, 5.3ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 416x640 1 Bear, 5.5ms\n",
      "Speed: 0.4ms preprocess, 5.5ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x512 1 Bear, 5.4ms\n",
      "Speed: 0.4ms preprocess, 5.4ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 Bear, 5.5ms\n",
      "Speed: 0.4ms preprocess, 5.5ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 2 Bears, 5.4ms\n",
      "Speed: 0.4ms preprocess, 5.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 5.3ms\n",
      "Speed: 0.4ms preprocess, 5.3ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/bd77266d2ab7abc6.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/bae4b1ffe740b1e8.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/2b13ba61c716676f.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/420b069ca1b7af5c.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/5237a8158f77e010.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/1fe0c16d50ed070c.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/90481efc08036315.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/43408607f3d6333c.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/0c9ba23768b0ceb9.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/3e900e9cd2a92ba1.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/0d3984a8a6b02283.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 448x640 1 Bear, 5.3ms\n",
      "Speed: 0.3ms preprocess, 5.3ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Bear, 5.4ms\n",
      "Speed: 0.4ms preprocess, 5.4ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 5.4ms\n",
      "Speed: 0.4ms preprocess, 5.4ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Bear, 5.3ms\n",
      "Speed: 0.3ms preprocess, 5.3ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 5.4ms\n",
      "Speed: 0.4ms preprocess, 5.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x448 1 Bear, 5.5ms\n",
      "Speed: 0.4ms preprocess, 5.5ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Bear, 5.6ms\n",
      "Speed: 0.4ms preprocess, 5.6ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x480 1 Bear, 5.5ms\n",
      "Speed: 0.4ms preprocess, 5.5ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 5.9ms\n",
      "Speed: 0.4ms preprocess, 5.9ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Bear, 5.5ms\n",
      "Speed: 0.4ms preprocess, 5.5ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 6.3ms\n",
      "Speed: 0.4ms preprocess, 6.3ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/834f743880624119.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/ddb8693b4be9dd46.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/d54dc3a3db1a370c.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/7cddf2ceb8ecddd7.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/0b202044b475f933.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/aad52fe29175881e.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/0f8e263ebdde6021.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/ef9b1114629bc475.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/2db4eff3174a528e.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/0c63dc22747b5b08.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/3490674d24d431fa.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 1 Bear, 5.9ms\n",
      "Speed: 0.4ms preprocess, 5.9ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Bear, 5.7ms\n",
      "Speed: 0.3ms preprocess, 5.7ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 6.8ms\n",
      "Speed: 0.3ms preprocess, 6.8ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 512x640 1 Bear, 5.9ms\n",
      "Speed: 0.4ms preprocess, 5.9ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 2 Bears, 5.9ms\n",
      "Speed: 0.3ms preprocess, 5.9ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Bear, 6.3ms\n",
      "Speed: 0.3ms preprocess, 6.3ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 5.1ms\n",
      "Speed: 0.3ms preprocess, 5.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 5.1ms\n",
      "Speed: 0.4ms preprocess, 5.1ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 4.8ms\n",
      "Speed: 0.3ms preprocess, 4.8ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 5.0ms\n",
      "Speed: 0.3ms preprocess, 5.0ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 4.9ms\n",
      "Speed: 0.4ms preprocess, 4.9ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/463312c9c96e8467.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/ed45056cc393342d.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/cd81c23d8d8fefeb.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/d3ffd44527772abb.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/cca289a80501817b.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/08ae2f319641862e.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/53588b11886f823d.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/4e5a72796536308b.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/a41088a517c4af0c.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/440bc70ff0ac3952.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/da0dd080084ba436.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 1 Bear, 5.3ms\n",
      "Speed: 0.3ms preprocess, 5.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 5.4ms\n",
      "Speed: 0.3ms preprocess, 5.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Bear, 5.5ms\n",
      "Speed: 0.4ms preprocess, 5.5ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 512x640 1 Bear, 5.4ms\n",
      "Speed: 0.4ms preprocess, 5.4ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 5.5ms\n",
      "Speed: 0.3ms preprocess, 5.5ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Bear, 5.7ms\n",
      "Speed: 0.4ms preprocess, 5.7ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 5.5ms\n",
      "Speed: 0.4ms preprocess, 5.5ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Bear, 5.6ms\n",
      "Speed: 0.4ms preprocess, 5.6ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 5.3ms\n",
      "Speed: 0.4ms preprocess, 5.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 5.2ms\n",
      "Speed: 0.4ms preprocess, 5.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Bear, 6.0ms\n",
      "Speed: 0.4ms preprocess, 6.0ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/68c183ec30152630.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/007e16be6a1fba04.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/3f1330007b93acd5.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/09b33ef8a1dcf147.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/4af7b4928d206930.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/216d01a6c5b1a9e3.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/e71c298bda9854d9.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/aa162089efb85adc.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/08edadfeb9e6ddc1.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/8c696f140130d9c9.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/4c24686fddc28b34.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 448x640 2 Bears, 5.9ms\n",
      "Speed: 0.4ms preprocess, 5.9ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 4.8ms\n",
      "Speed: 0.3ms preprocess, 4.8ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Bear, 5.4ms\n",
      "Speed: 0.4ms preprocess, 5.4ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 5.3ms\n",
      "Speed: 0.4ms preprocess, 5.3ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 5.0ms\n",
      "Speed: 0.4ms preprocess, 5.0ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 5.0ms\n",
      "Speed: 0.3ms preprocess, 5.0ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Bear, 5.5ms\n",
      "Speed: 0.4ms preprocess, 5.5ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x512 1 Bear, 5.4ms\n",
      "Speed: 0.4ms preprocess, 5.4ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 288x640 2 Bears, 21.8ms\n",
      "Speed: 0.3ms preprocess, 21.8ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 5.5ms\n",
      "Speed: 0.3ms preprocess, 5.5ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 5.1ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/2262342b3b1e2edf.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/21f3204dac15de53.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/292bcb1f76f2d7ae.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/c1aa80125f86bd73.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/174852382020da0f.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/891e0f709636fd8f.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/679aa0acbd4f3d44.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/97972189ab7d16a6.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/aebe7ee097aacd55.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/71c211cbe4efa586.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Speed: 0.4ms preprocess, 5.1ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 2 Bears, 8.9ms\n",
      "Speed: 0.4ms preprocess, 8.9ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 5.1ms\n",
      "Speed: 0.4ms preprocess, 5.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 1 Bear, 5.6ms\n",
      "Speed: 0.3ms preprocess, 5.6ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Bear, 5.5ms\n",
      "Speed: 0.4ms preprocess, 5.5ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 5.4ms\n",
      "Speed: 0.3ms preprocess, 5.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 Bears, 5.1ms\n",
      "Speed: 0.3ms preprocess, 5.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/096e1b2ab21d9988.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/e760a8f6a05846fa.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/2574cbced3014ae4.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/cbe96dcc42ce31b6.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/2783f713c6c8ebe9.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/1bd033f74de86d68.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/fba8b98beef33e00.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 512x640 1 Bear, 5.8ms\n",
      "Speed: 0.4ms preprocess, 5.8ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 Bears, 5.5ms\n",
      "Speed: 0.4ms preprocess, 5.5ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 5.4ms\n",
      "Speed: 0.4ms preprocess, 5.4ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 5.0ms\n",
      "Speed: 0.4ms preprocess, 5.0ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 5.1ms\n",
      "Speed: 0.3ms preprocess, 5.1ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 2 Bears, 5.5ms\n",
      "Speed: 0.4ms preprocess, 5.5ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 Bear, 5.0ms\n",
      "Speed: 0.4ms preprocess, 5.0ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 5.0ms\n",
      "Speed: 0.4ms preprocess, 5.0ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x544 1 Bear, 5.3ms\n",
      "Speed: 0.4ms preprocess, 5.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 9.0ms\n",
      "Speed: 0.4ms preprocess, 9.0ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/0093034e0b5d378e.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/1f59fdf9ba438103.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/08ae713666d0bf12.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/886bc3daf324c85b.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/7ea99ccf10a1d66f.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/227c69879c5e64ed.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/a304b04d5c40279c.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/497c470efbab6a43.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/3105d086e08320cd.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/f6d343dc3959b586.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x640 1 Bear, 5.3ms\n",
      "Speed: 0.4ms preprocess, 5.3ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 5.5ms\n",
      "Speed: 0.4ms preprocess, 5.5ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 Bear, 5.4ms\n",
      "Speed: 0.4ms preprocess, 5.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 Bear, 4.9ms\n",
      "Speed: 0.4ms preprocess, 4.9ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 5.4ms\n",
      "Speed: 0.4ms preprocess, 5.4ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 Bear, 5.5ms\n",
      "Speed: 0.4ms preprocess, 5.5ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 416x640 1 Bear, 5.1ms\n",
      "Speed: 0.3ms preprocess, 5.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 5.5ms\n",
      "Speed: 0.4ms preprocess, 5.5ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 5.1ms\n",
      "Speed: 0.4ms preprocess, 5.1ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 544x640 1 Bear, 7.2ms\n",
      "Speed: 0.4ms preprocess, 7.2ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/4fe21b0c8f45ce7f.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/31fdbb2b35a67808.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/611aff3762646348.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/81ed5e079de7852e.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/964143ae4c7c84fe.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/181d6c2439ddebf8.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/7127e183220faa23.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/0574cfdd1f8bfaa6.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/16e103df6d031405.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/aeaada8302955f30.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 1 Bear, 5.7ms\n",
      "Speed: 0.4ms preprocess, 5.7ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Bear, 5.2ms\n",
      "Speed: 0.4ms preprocess, 5.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Bear, 6.1ms\n",
      "Speed: 0.4ms preprocess, 6.1ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 5.3ms\n",
      "Speed: 0.4ms preprocess, 5.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 5.0ms\n",
      "Speed: 0.3ms preprocess, 5.0ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Bear, 5.5ms\n",
      "Speed: 0.3ms preprocess, 5.5ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 5.3ms\n",
      "Speed: 0.3ms preprocess, 5.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Bear, 5.6ms\n",
      "Speed: 0.4ms preprocess, 5.6ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 5.4ms\n",
      "Speed: 0.3ms preprocess, 5.4ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 4.9ms\n",
      "Speed: 0.3ms preprocess, 4.9ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Bear, 5.5ms\n",
      "Speed: 0.3ms preprocess, 5.5ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Bear, 5.2ms\n",
      "Speed: 0.4ms preprocess, 5.2ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/550917186ef95098.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/6933da5063388a18.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/3f06b197e78831a1.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/47b556732c243409.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/96f556ec3503720c.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/0f5a074593aa6e6c.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/72cb7fe029783d8c.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/509a1f098ba5fdca.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/798572c8f5c0a99f.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/1bd0e1d7ec1e6d9a.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/d6f68f3cac7dd5c1.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/5a4f7ddf94962312.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 448x640 1 Bear, 5.3ms\n",
      "Speed: 0.3ms preprocess, 5.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 5.0ms\n",
      "Speed: 0.4ms preprocess, 5.0ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x480 1 Bear, 5.5ms\n",
      "Speed: 0.4ms preprocess, 5.5ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Bear, 6.9ms\n",
      "Speed: 0.4ms preprocess, 6.9ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 7.3ms\n",
      "Speed: 0.4ms preprocess, 7.3ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 5.2ms\n",
      "Speed: 0.3ms preprocess, 5.2ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x512 1 Bear, 5.6ms\n",
      "Speed: 0.3ms preprocess, 5.6ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 5.4ms\n",
      "Speed: 0.4ms preprocess, 5.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 5.0ms\n",
      "Speed: 0.4ms preprocess, 5.0ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 5.2ms\n",
      "Speed: 0.3ms preprocess, 5.2ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 4.9ms\n",
      "Speed: 0.4ms preprocess, 4.9ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/94c5230aa892a9b3.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/abaf41c95f7f275f.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/a5dbe38f9148c093.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/3893b0695cd4f11d.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/01bace6c9e411e90.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/960c21ca505189f2.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/934f65ab60036932.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/c88f33f30c4354a3.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/cfe70e879cd26021.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/a118104a129e91a7.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/3a43a639fc14bdb9.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 1 Bear, 5.7ms\n",
      "Speed: 0.4ms preprocess, 5.7ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 8.6ms\n",
      "Speed: 0.3ms preprocess, 8.6ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Bear, 5.6ms\n",
      "Speed: 0.4ms preprocess, 5.6ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 5.4ms\n",
      "Speed: 0.3ms preprocess, 5.4ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Bear, 5.2ms\n",
      "Speed: 0.4ms preprocess, 5.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 5.2ms\n",
      "Speed: 0.3ms preprocess, 5.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 2 Bears, 6.1ms\n",
      "Speed: 0.4ms preprocess, 6.1ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 5.2ms\n",
      "Speed: 0.4ms preprocess, 5.2ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Bear, 5.6ms\n",
      "Speed: 0.4ms preprocess, 5.6ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 5.3ms\n",
      "Speed: 0.4ms preprocess, 5.3ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/0b99e0ce2836c0a2.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/defa9f9981d00026.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/224ff2aabe5aa4e9.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/6898749e30bdb849.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/0b73ea8f94bad603.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/fceace889ee9085c.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/3387f3531c8602cf.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/cc16a2b97d69d596.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/89132bf58464c641.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/82a2022d697b9dfe.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 448x640 1 Bear, 5.3ms\n",
      "Speed: 0.4ms preprocess, 5.3ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 7.4ms\n",
      "Speed: 0.4ms preprocess, 7.4ms inference, 2.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 5.3ms\n",
      "Speed: 0.4ms preprocess, 5.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 5.0ms\n",
      "Speed: 0.3ms preprocess, 5.0ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x480 1 Bear, 5.5ms\n",
      "Speed: 0.4ms preprocess, 5.5ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 6.6ms\n",
      "Speed: 0.3ms preprocess, 6.6ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Bear, 5.5ms\n",
      "Speed: 0.3ms preprocess, 5.5ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Bear, 4.9ms\n",
      "Speed: 0.4ms preprocess, 4.9ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Bear, 5.0ms\n",
      "Speed: 0.4ms preprocess, 5.0ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Bear, 5.0ms\n",
      "Speed: 0.4ms preprocess, 5.0ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 Bears, 5.7ms\n",
      "Speed: 0.4ms preprocess, 5.7ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/4d1abfceceaa6d8f.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/cd986bf54763c632.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/81c8366e07a49550.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/0fcbde3538f436ba.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/cbc4abaec09a240a.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/55bb7ecb0ee8e98c.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/553dc6a8ad447d58.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/2e00705e98ac9a57.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/e947857b0175e07c.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/75023a8b893c7668.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/1aa7251f00cb54d4.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 1 Bear, 5.5ms\n",
      "Speed: 0.4ms preprocess, 5.5ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 5.4ms\n",
      "Speed: 0.4ms preprocess, 5.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 5.0ms\n",
      "Speed: 0.4ms preprocess, 5.0ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 5.0ms\n",
      "Speed: 0.3ms preprocess, 5.0ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Bear, 5.8ms\n",
      "Speed: 0.4ms preprocess, 5.8ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 5.4ms\n",
      "Speed: 0.4ms preprocess, 5.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 2 Bears, 5.2ms\n",
      "Speed: 0.4ms preprocess, 5.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 Bear, 5.5ms\n",
      "Speed: 0.4ms preprocess, 5.5ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 6.3ms\n",
      "Speed: 0.3ms preprocess, 6.3ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 1 Bear, 5.9ms\n",
      "Speed: 0.3ms preprocess, 5.9ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 5.7ms\n",
      "Speed: 0.3ms preprocess, 5.7ms inference, 2.7ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/2ed17656a161b2a1.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/9a2db4d41bb89952.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/da20d9fbaf21bd0c.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/936fe63c7f30da30.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/52a6918b073eab74.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/6092358b5df424df.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/1a0a81c2adc7afd3.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/0eee4ccfefaa6b65.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/86ee138a6bb66ca6.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/068f3085bcf99f7b.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 Bear, 5.8ms\n",
      "Speed: 0.3ms preprocess, 5.8ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 512x640 1 Bear, 5.4ms\n",
      "Speed: 0.4ms preprocess, 5.4ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 Bear, 5.4ms\n",
      "Speed: 0.4ms preprocess, 5.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 5.4ms\n",
      "Speed: 0.4ms preprocess, 5.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Bear, 5.5ms\n",
      "Speed: 0.4ms preprocess, 5.5ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 5.6ms\n",
      "Speed: 0.3ms preprocess, 5.6ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 Bears, 5.4ms\n",
      "Speed: 0.3ms preprocess, 5.4ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 512x640 1 Bear, 5.8ms\n",
      "Speed: 0.4ms preprocess, 5.8ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 5.4ms\n",
      "Speed: 0.4ms preprocess, 5.4ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 4.8ms\n",
      "Speed: 0.4ms preprocess, 4.8ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/396f59fdeac379b9.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/d9861e21277f318d.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/38bd55103814c901.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/91404f8faffaa38a.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/51051700ea8c26c8.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/22c32f4044761029.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/52ef901c1f34ba82.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/892bf13006b5c81a.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/bb75276b6f925fca.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/0dd512b5bcd2a99d.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/6326f70af57ed2b5.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x448 1 Bear, 5.1ms\n",
      "Speed: 0.3ms preprocess, 5.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Bear, 5.3ms\n",
      "Speed: 0.3ms preprocess, 5.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 5.3ms\n",
      "Speed: 0.4ms preprocess, 5.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 4.8ms\n",
      "Speed: 0.3ms preprocess, 4.8ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Bear, 5.3ms\n",
      "Speed: 0.3ms preprocess, 5.3ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 5.4ms\n",
      "Speed: 0.3ms preprocess, 5.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 5.0ms\n",
      "Speed: 0.3ms preprocess, 5.0ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Bear, 5.5ms\n",
      "Speed: 0.4ms preprocess, 5.5ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 5.6ms\n",
      "Speed: 0.4ms preprocess, 5.6ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 5.1ms\n",
      "Speed: 0.4ms preprocess, 5.1ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x512 1 Bear, 5.6ms\n",
      "Speed: 0.4ms preprocess, 5.6ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Bear, 5.4ms\n",
      "Speed: 0.4ms preprocess, 5.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/c76801fcc1810b72.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/059a45a42b3c1c87.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/60fa8c8d8139b21b.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/5c167989a4a89486.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/ed4378ed2ef65a95.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/db57a9cf321ee7ba.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/45121b3a1659e429.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/da2a7feea17f1828.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/7aad5ba648887393.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/f1797ec2f19d50cb.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/c8c2752735613daa.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/47cf85744a06e964.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 Bear, 5.0ms\n",
      "Speed: 0.3ms preprocess, 5.0ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 576x640 1 Bear, 5.4ms\n",
      "Speed: 0.4ms preprocess, 5.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Bear, 5.7ms\n",
      "Speed: 0.4ms preprocess, 5.7ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 544x640 1 Bear, 5.0ms\n",
      "Speed: 0.4ms preprocess, 5.0ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 512x640 1 Bear, 5.3ms\n",
      "Speed: 0.4ms preprocess, 5.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 5.4ms\n",
      "Speed: 0.4ms preprocess, 5.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 5.1ms\n",
      "Speed: 0.4ms preprocess, 5.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Bear, 5.5ms\n",
      "Speed: 0.4ms preprocess, 5.5ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 1 Bear, 5.5ms\n",
      "Speed: 0.3ms preprocess, 5.5ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 5.1ms\n",
      "Speed: 0.4ms preprocess, 5.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x448 1 Bear, 5.4ms\n",
      "Speed: 0.4ms preprocess, 5.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/3c18ee488c17770b.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/c2bbd278e0a1560a.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/aa7ae53dd481be90.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/0c59f68e94083b87.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/023058ec02eb762e.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/220c8eeec66bae4f.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/1b6a0e26cc906308.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/1d990d673b3b7870.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/5b900d7e4fdd958e.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/16559978b8293674.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 448x640 1 Bear, 5.4ms\n",
      "Speed: 0.3ms preprocess, 5.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Bear, 6.3ms\n",
      "Speed: 0.6ms preprocess, 6.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Bear, 5.0ms\n",
      "Speed: 0.4ms preprocess, 5.0ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Bear, 5.1ms\n",
      "Speed: 0.4ms preprocess, 5.1ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Bear, 5.0ms\n",
      "Speed: 0.4ms preprocess, 5.0ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 5.1ms\n",
      "Speed: 0.4ms preprocess, 5.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 4.8ms\n",
      "Speed: 0.4ms preprocess, 4.8ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Bear, 5.1ms\n",
      "Speed: 0.4ms preprocess, 5.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 544x640 1 Bear, 5.3ms\n",
      "Speed: 0.4ms preprocess, 5.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 5.2ms\n",
      "Speed: 0.4ms preprocess, 5.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/74a3a6c8f83a13be.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/fcb936908440c794.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/9865810c87999088.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/0ba1b5eb886a408a.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/304368a2fbb51d00.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/28975a26b57024ed.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/8a38d92febcb82a8.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/1faf27fd9940a1c1.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/1aaaefd3234ff51e.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/242fb6fb9d80b4b5.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/8315203d3aa75b21.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 Bear, 5.3ms\n",
      "Speed: 0.3ms preprocess, 5.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 5.4ms\n",
      "Speed: 0.4ms preprocess, 5.4ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x512 1 Bear, 5.4ms\n",
      "Speed: 0.4ms preprocess, 5.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 5.4ms\n",
      "Speed: 0.4ms preprocess, 5.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x448 1 Bear, 5.3ms\n",
      "Speed: 0.4ms preprocess, 5.3ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 5.4ms\n",
      "Speed: 0.4ms preprocess, 5.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Bear, 5.4ms\n",
      "Speed: 0.4ms preprocess, 5.4ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 5.6ms\n",
      "Speed: 0.4ms preprocess, 5.6ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Bear, 5.5ms\n",
      "Speed: 0.4ms preprocess, 5.5ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Bear, 5.0ms\n",
      "Speed: 0.4ms preprocess, 5.0ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/1d568c8d71d44faa.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/9120a9df74d2e136.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/66fdb49ed24ea1f4.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/0ebe43ee23e535fe.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/fd5a0280e9052bb9.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/ac0de0e5da98dffc.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/1680236290eff77a.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/a92a220c72d0c510.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/3c4bc54ec9462bdc.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/d18ab6c9aaa64d92.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 640x384 1 Bear, 24.0ms\n",
      "Speed: 0.4ms preprocess, 24.0ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 5.1ms\n",
      "Speed: 0.3ms preprocess, 5.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 4.6ms\n",
      "Speed: 0.4ms preprocess, 4.6ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 4.7ms\n",
      "Speed: 0.3ms preprocess, 4.7ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 544x640 1 Bear, 5.3ms\n",
      "Speed: 0.4ms preprocess, 5.3ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Bear, 5.7ms\n",
      "Speed: 0.4ms preprocess, 5.7ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 2 Bears, 5.7ms\n",
      "Speed: 0.4ms preprocess, 5.7ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x544 1 Bear, 5.1ms\n",
      "Speed: 0.4ms preprocess, 5.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 5.3ms\n",
      "Speed: 0.4ms preprocess, 5.3ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 5.2ms\n",
      "Speed: 0.3ms preprocess, 5.2ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 Bear, 5.4ms\n",
      "Speed: 0.4ms preprocess, 5.4ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/1c8109d39473f82a.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/7e4076e5ece14631.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/620dbb6ee89c8786.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/46f40a06c761c82f.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/b432268f14f221e7.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/359ddcd3297621c9.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/3a8dbb014a4ffbc5.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/22b0b0338f8b0532.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/9d28797707b94492.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/33ba5c5495fc0dce.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/0d0838106bf03bd0.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 1 Bear, 8.5ms\n",
      "Speed: 0.3ms preprocess, 8.5ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 576x640 1 Bear, 5.2ms\n",
      "Speed: 0.4ms preprocess, 5.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x512 1 Bear, 5.1ms\n",
      "Speed: 0.4ms preprocess, 5.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 416x640 1 Bear, 5.8ms\n",
      "Speed: 0.3ms preprocess, 5.8ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 5.4ms\n",
      "Speed: 0.4ms preprocess, 5.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Bear, 5.5ms\n",
      "Speed: 0.3ms preprocess, 5.5ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 Bear, 5.1ms\n",
      "Speed: 0.4ms preprocess, 5.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 5.5ms\n",
      "Speed: 0.4ms preprocess, 5.5ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 1 Bear, 5.5ms\n",
      "Speed: 0.4ms preprocess, 5.5ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 544x640 1 Bear, 5.3ms\n",
      "Speed: 0.3ms preprocess, 5.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 5.1ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/4d9ca7f6ab228f78.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/31951d17a633487c.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/83c7c8a11307c2b0.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/c4c71942a1cba4f8.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/8da8b4f8a9259098.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/52ccfea17752e500.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/22774b671bc25ee1.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/376d788fe07f11a0.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/53808f22dfab8ea3.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/48da25a89f4af9ed.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Speed: 0.3ms preprocess, 5.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 5.1ms\n",
      "Speed: 0.4ms preprocess, 5.1ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 5.1ms\n",
      "Speed: 0.4ms preprocess, 5.1ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x480 1 Bear, 5.2ms\n",
      "Speed: 0.4ms preprocess, 5.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Bear, 5.7ms\n",
      "Speed: 0.4ms preprocess, 5.7ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 512x640 1 Bear, 5.4ms\n",
      "Speed: 0.4ms preprocess, 5.4ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 576x640 1 Bear, 5.2ms\n",
      "Speed: 0.4ms preprocess, 5.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 5.0ms\n",
      "Speed: 0.4ms preprocess, 5.0ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/c154a1c43a814754.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/483bdac69588d197.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/866934921e9794ae.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/0da7be6036d693b2.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/6a2e81967e187015.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/d2a70f086865cbb4.jpg\n",
      "/home/jupyter-yixuan/project_machine_learning/dataset3/images/train/0061debb2a202214.jpg\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "model_path = '/home/jupyter-yixuan/project_machine_learning/runs/segment/train9/weights/last.pt'\n",
    "\n",
    "# Load the YOLO model\n",
    "model = YOLO(model_path)\n",
    "\n",
    "\n",
    "# image_path = '/home/jupyter-yixuan/project_machine_learning/angry_bears/images/1.jpeg'\n",
    "for image_path in os.listdir(original_folder):\n",
    "    image_path = os.path.join(original_folder, image_path)\n",
    "    # Load the original image\n",
    "    print(image_path)\n",
    "    img = cv2.imread(image_path)\n",
    "    H, W, _ = img.shape\n",
    "    \n",
    "    # Perform inference to get the masks\n",
    "    results = model(img)\n",
    "\n",
    "    if results is not None:\n",
    "        # Initialize a blank canvas to store the filtered result\n",
    "        filtered_img = np.zeros_like(img)\n",
    "        \n",
    "        for result in results:\n",
    "            for j, mask in enumerate(result.masks.data):\n",
    "                # Convert the mask to a numpy array and scale to 255\n",
    "                mask = mask.cpu().numpy() * 255\n",
    "                \n",
    "                # Resize the mask to match the original image dimensions\n",
    "                mask = cv2.resize(mask, (W, H))\n",
    "                \n",
    "                # Ensure the mask is binary (0 or 255)\n",
    "                _, binary_mask = cv2.threshold(mask, 128, 255, cv2.THRESH_BINARY)\n",
    "                \n",
    "                # Apply the mask to the original image using bitwise operations\n",
    "                masked_img = cv2.bitwise_and(img, img, mask=binary_mask.astype(np.uint8))\n",
    "                \n",
    "                # Combine with the filtered image\n",
    "                filtered_img = cv2.add(filtered_img, masked_img)\n",
    "        \n",
    "        # Save the filtered image\n",
    "        new_path = os.path.join(destination_folder, image_path.split('/')[-1])\n",
    "        cv2.imwrite(new_path, filtered_img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4c43afc7-0139-4987-ad26-ed2b1167737d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "395"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(os.listdir(destination_folder))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965ac390-eb72-41f6-aa5f-d9cd01a7b79f",
   "metadata": {},
   "source": [
    "### ! Get the filtered images considering all exceptional cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "aba83e00-2bb3-4be8-86dc-0268c5c67177",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_folder = '/home/jupyter-yixuan/project_machine_learning/dataset3/images/val'\n",
    "destination_folder = '/home/jupyter-yixuan/project_machine_learning/dataset3_filtered/val_filtered'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d08b3d03-5462-4776-89ee-17b5ebe64e8c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 448x640 1 Bear, 4.7ms\n",
      "Speed: 0.3ms preprocess, 4.7ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 3 Bears, 4.7ms\n",
      "Speed: 0.4ms preprocess, 4.7ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 2 Bears, 5.0ms\n",
      "Speed: 0.4ms preprocess, 5.0ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Bear, 5.4ms\n",
      "Speed: 0.4ms preprocess, 5.4ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Bear, 5.2ms\n",
      "Speed: 0.4ms preprocess, 5.2ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Bear, 5.0ms\n",
      "Speed: 0.4ms preprocess, 5.0ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 5.3ms\n",
      "Speed: 0.4ms preprocess, 5.3ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Bear, 5.4ms\n",
      "Speed: 0.4ms preprocess, 5.4ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Bear, 5.2ms\n",
      "Speed: 0.4ms preprocess, 5.2ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 512x640 1 Bear, 5.2ms\n",
      "Speed: 0.4ms preprocess, 5.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 5.3ms\n",
      "Speed: 0.4ms preprocess, 5.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 Bear, 5.1ms\n",
      "Speed: 0.4ms preprocess, 5.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 5.4ms\n",
      "Speed: 0.4ms preprocess, 5.4ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Bear, 5.3ms\n",
      "Speed: 0.4ms preprocess, 5.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Bear, 5.1ms\n",
      "Speed: 0.4ms preprocess, 5.1ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 6.0ms\n",
      "Speed: 0.4ms preprocess, 6.0ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 1 Bear, 5.7ms\n",
      "Speed: 0.3ms preprocess, 5.7ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 6.8ms\n",
      "Speed: 0.4ms preprocess, 6.8ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 4.7ms\n",
      "Speed: 0.3ms preprocess, 4.7ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Bear, 5.4ms\n",
      "Speed: 0.4ms preprocess, 5.4ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 5.4ms\n",
      "Speed: 0.4ms preprocess, 5.4ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x480 1 Bear, 5.3ms\n",
      "Speed: 0.4ms preprocess, 5.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x448 2 Bears, 5.5ms\n",
      "Speed: 0.4ms preprocess, 5.5ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Bear, 5.5ms\n",
      "Speed: 0.4ms preprocess, 5.5ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 2 Bears, 4.8ms\n",
      "Speed: 0.4ms preprocess, 4.8ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Bear, 5.2ms\n",
      "Speed: 0.4ms preprocess, 5.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 416x640 2 Bears, 5.5ms\n",
      "Speed: 0.3ms preprocess, 5.5ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 (no detections), 5.5ms\n",
      "Speed: 0.3ms preprocess, 5.5ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 4.5ms\n",
      "Speed: 0.3ms preprocess, 4.5ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 2 Bears, 5.3ms\n",
      "Speed: 0.4ms preprocess, 5.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 5.1ms\n",
      "Speed: 0.3ms preprocess, 5.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Bear, 5.4ms\n",
      "Speed: 0.4ms preprocess, 5.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Bear, 5.0ms\n",
      "Speed: 0.4ms preprocess, 5.0ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 5.4ms\n",
      "Speed: 0.3ms preprocess, 5.4ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 2 Bears, 5.3ms\n",
      "Speed: 0.4ms preprocess, 5.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 5.3ms\n",
      "Speed: 0.4ms preprocess, 5.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Bear, 5.3ms\n",
      "Speed: 0.3ms preprocess, 5.3ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x448 1 Bear, 5.3ms\n",
      "Speed: 0.4ms preprocess, 5.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 2 Bears, 5.2ms\n",
      "Speed: 0.4ms preprocess, 5.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 5.0ms\n",
      "Speed: 0.4ms preprocess, 5.0ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 5.0ms\n",
      "Speed: 0.4ms preprocess, 5.0ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Bear, 5.4ms\n",
      "Speed: 0.4ms preprocess, 5.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 2 Bears, 5.4ms\n",
      "Speed: 0.4ms preprocess, 5.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 5.0ms\n",
      "Speed: 0.4ms preprocess, 5.0ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 512x640 1 Bear, 5.4ms\n",
      "Speed: 0.4ms preprocess, 5.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 5.8ms\n",
      "Speed: 0.4ms preprocess, 5.8ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Bear, 5.1ms\n",
      "Speed: 0.3ms preprocess, 5.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 5.4ms\n",
      "Speed: 0.4ms preprocess, 5.4ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Bear, 5.2ms\n",
      "Speed: 0.4ms preprocess, 5.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 576x640 1 Bear, 5.4ms\n",
      "Speed: 0.4ms preprocess, 5.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Bear, 5.5ms\n",
      "Speed: 0.4ms preprocess, 5.5ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 5.3ms\n",
      "Speed: 0.4ms preprocess, 5.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 5.0ms\n",
      "Speed: 0.4ms preprocess, 5.0ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Bear, 5.4ms\n",
      "Speed: 0.4ms preprocess, 5.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 1 Bear, 5.5ms\n",
      "Speed: 0.3ms preprocess, 5.5ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Bear, 5.4ms\n",
      "Speed: 0.3ms preprocess, 5.4ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 5.4ms\n",
      "Speed: 0.3ms preprocess, 5.4ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 512x640 1 Bear, 5.1ms\n",
      "Speed: 0.4ms preprocess, 5.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 5.4ms\n",
      "Speed: 0.4ms preprocess, 5.4ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x448 1 Bear, 5.4ms\n",
      "Speed: 0.3ms preprocess, 5.4ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 608x640 1 Bear, 5.1ms\n",
      "Speed: 0.4ms preprocess, 5.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Bear, 5.3ms\n",
      "Speed: 0.4ms preprocess, 5.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x480 2 Bears, 5.1ms\n",
      "Speed: 0.4ms preprocess, 5.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 416x640 2 Bears, 5.7ms\n",
      "Speed: 0.4ms preprocess, 5.7ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Bear, 5.3ms\n",
      "Speed: 0.4ms preprocess, 5.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 5.3ms\n",
      "Speed: 0.4ms preprocess, 5.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "model_path = '/home/jupyter-yixuan/project_machine_learning/runs/segment/train9/weights/last.pt'\n",
    "\n",
    "# Load the YOLO model\n",
    "model = YOLO(model_path)\n",
    "\n",
    "\n",
    "# image_path = '/home/jupyter-yixuan/project_machine_learning/angry_bears/images/1.jpeg'\n",
    "for image_path in os.listdir(original_folder):\n",
    "    extension = image_path.split('.')[-1]\n",
    "    if extension == 'jpg':    \n",
    "        image_path = os.path.join(original_folder, image_path)\n",
    "        # Load the original image\n",
    "        # print(image_path)\n",
    "        img = cv2.imread(image_path)\n",
    "        H, W, _ = img.shape\n",
    "        \n",
    "        # Perform inference to get the masks\n",
    "        results = model(img)\n",
    "    \n",
    "        # Initialize a blank canvas to store the filtered result\n",
    "        filtered_img = np.zeros_like(img)\n",
    "        \n",
    "        for result in results:\n",
    "            if result.masks is not None:\n",
    "                for j, mask in enumerate(result.masks.data):\n",
    "                    # Convert the mask to a numpy array and scale to 255\n",
    "                    mask = mask.cpu().numpy() * 255\n",
    "                    \n",
    "                    # Resize the mask to match the original image dimensions\n",
    "                    mask = cv2.resize(mask, (W, H))\n",
    "                    \n",
    "                    # Ensure the mask is binary (0 or 255)\n",
    "                    _, binary_mask = cv2.threshold(mask, 128, 255, cv2.THRESH_BINARY)\n",
    "                    \n",
    "                    # Apply the mask to the original image using bitwise operations\n",
    "                    masked_img = cv2.bitwise_and(img, img, mask=binary_mask.astype(np.uint8))\n",
    "                    \n",
    "                    # Combine with the filtered image\n",
    "                    filtered_img = cv2.add(filtered_img, masked_img)\n",
    "            \n",
    "            # Save the filtered image\n",
    "            new_path = os.path.join(destination_folder, image_path.split('/')[-1])\n",
    "            cv2.imwrite(new_path, filtered_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2603e8b0-7adb-42a8-818d-64990b034390",
   "metadata": {},
   "source": [
    "## 4 Segment angry bears images downloaded from Google using the trained YOLO model and get Filtered Images without Background"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ed8707-e771-463c-967e-de25c190310d",
   "metadata": {},
   "source": [
    "project_machine_learning/angry_bears/images\n",
    "project_machine_learning/angry_bears/masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ccbac527-28e7-4562-a415-4d599ef832dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_folder = '/home/jupyter-yixuan/project_machine_learning/angry_bears/images'\n",
    "destination_folder = '/home/jupyter-yixuan/project_machine_learning/angry_bears/masks2'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53105ea4-44e5-4433-8447-d3bba42b6828",
   "metadata": {},
   "source": [
    "Do not check  \"if results is not None:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "39a85682-931d-42ea-99ef-a90d2b633e19",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 448x640 1 Bear, 4.8ms\n",
      "Speed: 0.2ms preprocess, 4.8ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 Bear, 5.0ms\n",
      "Speed: 0.3ms preprocess, 5.0ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Bear, 5.2ms\n",
      "Speed: 0.3ms preprocess, 5.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x544 1 Bear, 5.5ms\n",
      "Speed: 0.3ms preprocess, 5.5ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 5.3ms\n",
      "Speed: 0.3ms preprocess, 5.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 1 Bear, 5.3ms\n",
      "Speed: 0.2ms preprocess, 5.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x512 1 Bear, 4.7ms\n",
      "Speed: 0.3ms preprocess, 4.7ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x448 1 Bear, 5.1ms\n",
      "Speed: 0.2ms preprocess, 5.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x544 1 Bear, 5.1ms\n",
      "Speed: 0.3ms preprocess, 5.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 5.3ms\n",
      "Speed: 0.3ms preprocess, 5.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 4.9ms\n",
      "Speed: 0.3ms preprocess, 4.9ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 5.0ms\n",
      "Speed: 0.3ms preprocess, 5.0ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter-yixuan/project_machine_learning/angry_bears/images/32.jpeg\n",
      "/home/jupyter-yixuan/project_machine_learning/angry_bears/images/72.jpeg\n",
      "/home/jupyter-yixuan/project_machine_learning/angry_bears/images/14.jpeg\n",
      "/home/jupyter-yixuan/project_machine_learning/angry_bears/images/26.jpeg\n",
      "/home/jupyter-yixuan/project_machine_learning/angry_bears/images/70.jpeg\n",
      "/home/jupyter-yixuan/project_machine_learning/angry_bears/images/89.jpeg\n",
      "/home/jupyter-yixuan/project_machine_learning/angry_bears/images/52.jpeg\n",
      "/home/jupyter-yixuan/project_machine_learning/angry_bears/images/4.jpeg\n",
      "/home/jupyter-yixuan/project_machine_learning/angry_bears/images/82.jpeg\n",
      "/home/jupyter-yixuan/project_machine_learning/angry_bears/images/34.jpeg\n",
      "/home/jupyter-yixuan/project_machine_learning/angry_bears/images/29.jpeg\n",
      "/home/jupyter-yixuan/project_machine_learning/angry_bears/images/1.jpeg\n",
      "/home/jupyter-yixuan/project_machine_learning/angry_bears/images/22.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 544x640 1 Bear, 5.0ms\n",
      "Speed: 0.2ms preprocess, 5.0ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Bear, 5.2ms\n",
      "Speed: 0.2ms preprocess, 5.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 1 Bear, 4.8ms\n",
      "Speed: 0.2ms preprocess, 4.8ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Bear, 5.5ms\n",
      "Speed: 0.3ms preprocess, 5.5ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 Bear, 5.2ms\n",
      "Speed: 0.3ms preprocess, 5.2ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 5.1ms\n",
      "Speed: 0.2ms preprocess, 5.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 Bear, 5.1ms\n",
      "Speed: 0.3ms preprocess, 5.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 1 Bear, 5.0ms\n",
      "Speed: 0.2ms preprocess, 5.0ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x416 1 Bear, 5.0ms\n",
      "Speed: 0.2ms preprocess, 5.0ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 5.0ms\n",
      "Speed: 0.2ms preprocess, 5.0ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 4.3ms\n",
      "Speed: 0.2ms preprocess, 4.3ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 1 Bear, 5.0ms\n",
      "Speed: 0.2ms preprocess, 5.0ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 320x640 1 Bear, 5.1ms\n",
      "Speed: 0.2ms preprocess, 5.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 512x640 1 Bear, 5.1ms\n",
      "Speed: 0.2ms preprocess, 5.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 544x640 1 Bear, 5.0ms\n",
      "Speed: 0.2ms preprocess, 5.0ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 1 Bear, 5.1ms\n",
      "Speed: 0.2ms preprocess, 5.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 1 Bear, 5.1ms\n",
      "Speed: 0.2ms preprocess, 5.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Bear, 5.1ms\n",
      "Speed: 0.2ms preprocess, 5.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 1 Bear, 4.9ms\n",
      "Speed: 0.2ms preprocess, 4.9ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 1 Bear, 4.3ms\n",
      "Speed: 0.2ms preprocess, 4.3ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x480 1 Bear, 5.0ms\n",
      "Speed: 0.2ms preprocess, 5.0ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter-yixuan/project_machine_learning/angry_bears/images/56.jpeg\n",
      "/home/jupyter-yixuan/project_machine_learning/angry_bears/images/84.jpeg\n",
      "/home/jupyter-yixuan/project_machine_learning/angry_bears/images/42.jpeg\n",
      "/home/jupyter-yixuan/project_machine_learning/angry_bears/images/19.jpeg\n",
      "/home/jupyter-yixuan/project_machine_learning/angry_bears/images/27.jpeg\n",
      "/home/jupyter-yixuan/project_machine_learning/angry_bears/images/76.jpeg\n",
      "/home/jupyter-yixuan/project_machine_learning/angry_bears/images/69.jpeg\n",
      "/home/jupyter-yixuan/project_machine_learning/angry_bears/images/33.jpeg\n",
      "/home/jupyter-yixuan/project_machine_learning/angry_bears/images/92.jpeg\n",
      "/home/jupyter-yixuan/project_machine_learning/angry_bears/images/59.jpeg\n",
      "/home/jupyter-yixuan/project_machine_learning/angry_bears/images/12.jpeg\n",
      "/home/jupyter-yixuan/project_machine_learning/angry_bears/images/60.jpeg\n",
      "/home/jupyter-yixuan/project_machine_learning/angry_bears/images/5.jpeg\n",
      "/home/jupyter-yixuan/project_machine_learning/angry_bears/images/20.jpeg\n",
      "/home/jupyter-yixuan/project_machine_learning/angry_bears/images/87.jpeg\n",
      "/home/jupyter-yixuan/project_machine_learning/angry_bears/images/39.jpeg\n",
      "/home/jupyter-yixuan/project_machine_learning/angry_bears/images/47.jpeg\n",
      "/home/jupyter-yixuan/project_machine_learning/angry_bears/images/66.jpeg\n",
      "/home/jupyter-yixuan/project_machine_learning/angry_bears/images/81.jpeg\n",
      "/home/jupyter-yixuan/project_machine_learning/angry_bears/images/64.jpeg\n",
      "/home/jupyter-yixuan/project_machine_learning/angry_bears/images/50.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 640x640 1 Bear, 5.1ms\n",
      "Speed: 0.3ms preprocess, 5.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 1 Bear, 5.0ms\n",
      "Speed: 0.2ms preprocess, 5.0ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x448 1 Bear, 5.1ms\n",
      "Speed: 0.2ms preprocess, 5.1ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x416 1 Bear, 5.2ms\n",
      "Speed: 0.2ms preprocess, 5.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 Bears, 5.2ms\n",
      "Speed: 0.2ms preprocess, 5.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 1 Bear, 4.7ms\n",
      "Speed: 0.2ms preprocess, 4.7ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 5.0ms\n",
      "Speed: 0.2ms preprocess, 5.0ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 1 Bear, 5.0ms\n",
      "Speed: 0.2ms preprocess, 5.0ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 Bear, 5.0ms\n",
      "Speed: 0.3ms preprocess, 5.0ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 4.7ms\n",
      "Speed: 0.2ms preprocess, 4.7ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 576x640 1 Bear, 5.0ms\n",
      "Speed: 0.3ms preprocess, 5.0ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 5.1ms\n",
      "Speed: 0.2ms preprocess, 5.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 8.7ms\n",
      "Speed: 0.3ms preprocess, 8.7ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Bear, 5.1ms\n",
      "Speed: 0.3ms preprocess, 5.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Bear, 5.4ms\n",
      "Speed: 0.3ms preprocess, 5.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 544x640 1 Bear, 5.2ms\n",
      "Speed: 0.3ms preprocess, 5.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x480 1 Bear, 5.1ms\n",
      "Speed: 0.2ms preprocess, 5.1ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 1 Bear, 4.7ms\n",
      "Speed: 0.2ms preprocess, 4.7ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 5.0ms\n",
      "Speed: 0.2ms preprocess, 5.0ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 4.8ms\n",
      "Speed: 0.2ms preprocess, 4.8ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter-yixuan/project_machine_learning/angry_bears/images/65.jpeg\n",
      "/home/jupyter-yixuan/project_machine_learning/angry_bears/images/53.jpeg\n",
      "/home/jupyter-yixuan/project_machine_learning/angry_bears/images/74.jpeg\n",
      "/home/jupyter-yixuan/project_machine_learning/angry_bears/images/73.jpeg\n",
      "/home/jupyter-yixuan/project_machine_learning/angry_bears/images/54.jpeg\n",
      "/home/jupyter-yixuan/project_machine_learning/angry_bears/images/18.jpeg\n",
      "/home/jupyter-yixuan/project_machine_learning/angry_bears/images/83.jpeg\n",
      "/home/jupyter-yixuan/project_machine_learning/angry_bears/images/63.jpeg\n",
      "/home/jupyter-yixuan/project_machine_learning/angry_bears/images/61.jpeg\n",
      "/home/jupyter-yixuan/project_machine_learning/angry_bears/images/85.jpeg\n",
      "/home/jupyter-yixuan/project_machine_learning/angry_bears/images/15.jpeg\n",
      "/home/jupyter-yixuan/project_machine_learning/angry_bears/images/40.jpeg\n",
      "/home/jupyter-yixuan/project_machine_learning/angry_bears/images/91.jpeg\n",
      "/home/jupyter-yixuan/project_machine_learning/angry_bears/images/36.jpeg\n",
      "/home/jupyter-yixuan/project_machine_learning/angry_bears/images/45.jpeg\n",
      "/home/jupyter-yixuan/project_machine_learning/angry_bears/images/94.jpeg\n",
      "/home/jupyter-yixuan/project_machine_learning/angry_bears/images/75.jpeg\n",
      "/home/jupyter-yixuan/project_machine_learning/angry_bears/images/35.jpeg\n",
      "/home/jupyter-yixuan/project_machine_learning/angry_bears/images/10.jpeg\n",
      "/home/jupyter-yixuan/project_machine_learning/angry_bears/images/3.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 416x640 1 Bear, 5.5ms\n",
      "Speed: 0.2ms preprocess, 5.5ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x576 1 Bear, 5.1ms\n",
      "Speed: 0.2ms preprocess, 5.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 Bear, 5.1ms\n",
      "Speed: 0.3ms preprocess, 5.1ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 4.8ms\n",
      "Speed: 0.2ms preprocess, 4.8ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x480 1 Bear, 5.0ms\n",
      "Speed: 0.2ms preprocess, 5.0ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Bear, 5.0ms\n",
      "Speed: 0.2ms preprocess, 5.0ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x544 1 Bear, 5.2ms\n",
      "Speed: 0.2ms preprocess, 5.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x448 1 Bear, 6.3ms\n",
      "Speed: 0.2ms preprocess, 6.3ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 1 Bear, 6.0ms\n",
      "Speed: 0.2ms preprocess, 6.0ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 4.8ms\n",
      "Speed: 0.2ms preprocess, 4.8ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 416x640 1 Bear, 5.2ms\n",
      "Speed: 0.2ms preprocess, 5.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 5.1ms\n",
      "Speed: 0.2ms preprocess, 5.1ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 4.9ms\n",
      "Speed: 0.2ms preprocess, 4.9ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 5.0ms\n",
      "Speed: 0.3ms preprocess, 5.0ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Bear, 5.2ms\n",
      "Speed: 0.2ms preprocess, 5.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 1 Bear, 4.8ms\n",
      "Speed: 0.2ms preprocess, 4.8ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x448 1 Bear, 5.0ms\n",
      "Speed: 0.2ms preprocess, 5.0ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Bear, 5.1ms\n",
      "Speed: 0.3ms preprocess, 5.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 2 Bears, 5.1ms\n",
      "Speed: 0.2ms preprocess, 5.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 288x640 1 Bear, 5.2ms\n",
      "Speed: 0.2ms preprocess, 5.2ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter-yixuan/project_machine_learning/angry_bears/images/95.jpeg\n",
      "/home/jupyter-yixuan/project_machine_learning/angry_bears/images/41.jpeg\n",
      "/home/jupyter-yixuan/project_machine_learning/angry_bears/images/7.jpeg\n",
      "/home/jupyter-yixuan/project_machine_learning/angry_bears/images/78.jpeg\n",
      "/home/jupyter-yixuan/project_machine_learning/angry_bears/images/30.jpeg\n",
      "/home/jupyter-yixuan/project_machine_learning/angry_bears/images/55.jpeg\n",
      "/home/jupyter-yixuan/project_machine_learning/angry_bears/images/11.jpeg\n",
      "/home/jupyter-yixuan/project_machine_learning/angry_bears/images/86.jpeg\n",
      "/home/jupyter-yixuan/project_machine_learning/angry_bears/images/6.jpeg\n",
      "/home/jupyter-yixuan/project_machine_learning/angry_bears/images/38.jpeg\n",
      "/home/jupyter-yixuan/project_machine_learning/angry_bears/images/79.jpeg\n",
      "/home/jupyter-yixuan/project_machine_learning/angry_bears/images/8.jpeg\n",
      "/home/jupyter-yixuan/project_machine_learning/angry_bears/images/88.jpeg\n",
      "/home/jupyter-yixuan/project_machine_learning/angry_bears/images/28.jpeg\n",
      "/home/jupyter-yixuan/project_machine_learning/angry_bears/images/23.jpeg\n",
      "/home/jupyter-yixuan/project_machine_learning/angry_bears/images/24.jpeg\n",
      "/home/jupyter-yixuan/project_machine_learning/angry_bears/images/93.jpeg\n",
      "/home/jupyter-yixuan/project_machine_learning/angry_bears/images/77.jpeg\n",
      "/home/jupyter-yixuan/project_machine_learning/angry_bears/images/80.jpeg\n",
      "/home/jupyter-yixuan/project_machine_learning/angry_bears/images/2.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 448x640 1 Bear, 5.3ms\n",
      "Speed: 0.2ms preprocess, 5.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 Bear, 4.9ms\n",
      "Speed: 0.3ms preprocess, 4.9ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Bear, 5.1ms\n",
      "Speed: 0.2ms preprocess, 5.1ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 1 Bear, 8.0ms\n",
      "Speed: 0.2ms preprocess, 8.0ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 4.7ms\n",
      "Speed: 0.2ms preprocess, 4.7ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 1 Bear, 5.0ms\n",
      "Speed: 0.2ms preprocess, 5.0ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 2 Bears, 5.2ms\n",
      "Speed: 0.2ms preprocess, 5.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 416x640 1 Bear, 5.3ms\n",
      "Speed: 0.2ms preprocess, 5.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 5.0ms\n",
      "Speed: 0.2ms preprocess, 5.0ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x416 1 Bear, 4.7ms\n",
      "Speed: 0.2ms preprocess, 4.7ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 4.9ms\n",
      "Speed: 0.2ms preprocess, 4.9ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 4.7ms\n",
      "Speed: 0.3ms preprocess, 4.7ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 4.6ms\n",
      "Speed: 0.3ms preprocess, 4.6ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x480 1 Bear, 5.1ms\n",
      "Speed: 0.3ms preprocess, 5.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 Bear, 5.1ms\n",
      "Speed: 0.3ms preprocess, 5.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 5.1ms\n",
      "Speed: 0.3ms preprocess, 5.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter-yixuan/project_machine_learning/angry_bears/images/67.jpeg\n",
      "/home/jupyter-yixuan/project_machine_learning/angry_bears/images/58.jpeg\n",
      "/home/jupyter-yixuan/project_machine_learning/angry_bears/images/37.jpeg\n",
      "/home/jupyter-yixuan/project_machine_learning/angry_bears/images/43.jpeg\n",
      "/home/jupyter-yixuan/project_machine_learning/angry_bears/images/21.jpeg\n",
      "/home/jupyter-yixuan/project_machine_learning/angry_bears/images/17.jpeg\n",
      "/home/jupyter-yixuan/project_machine_learning/angry_bears/images/9.jpeg\n",
      "/home/jupyter-yixuan/project_machine_learning/angry_bears/images/96.jpeg\n",
      "/home/jupyter-yixuan/project_machine_learning/angry_bears/images/90.jpeg\n",
      "/home/jupyter-yixuan/project_machine_learning/angry_bears/images/31.jpeg\n",
      "/home/jupyter-yixuan/project_machine_learning/angry_bears/images/51.jpeg\n",
      "/home/jupyter-yixuan/project_machine_learning/angry_bears/images/46.jpeg\n",
      "/home/jupyter-yixuan/project_machine_learning/angry_bears/images/13.jpeg\n",
      "/home/jupyter-yixuan/project_machine_learning/angry_bears/images/71.jpeg\n",
      "/home/jupyter-yixuan/project_machine_learning/angry_bears/images/49.jpeg\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "model_path = '/home/jupyter-yixuan/project_machine_learning/runs/segment/train9/weights/last.pt'\n",
    "\n",
    "# Load the YOLO model\n",
    "model = YOLO(model_path)\n",
    "\n",
    "\n",
    "# image_path = '/home/jupyter-yixuan/project_machine_learning/angry_bears/images/1.jpeg'\n",
    "for image_path in os.listdir(original_folder):\n",
    "    \n",
    "    extension = image_path.split('.')[-1]\n",
    "    if extension == 'jpeg':   \n",
    "        image_path = os.path.join(original_folder, image_path)\n",
    "        # Load the original image\n",
    "        print(image_path)\n",
    "        img = cv2.imread(image_path)\n",
    "        H, W, _ = img.shape\n",
    "        \n",
    "        # Perform inference to get the masks\n",
    "        results = model(img)\n",
    "    \n",
    "        # Initialize a blank canvas to store the filtered result\n",
    "        filtered_img = np.zeros_like(img)\n",
    "        \n",
    "        for result in results:\n",
    "            for j, mask in enumerate(result.masks.data):\n",
    "                # Convert the mask to a numpy array and scale to 255\n",
    "                mask = mask.cpu().numpy() * 255\n",
    "                \n",
    "                # Resize the mask to match the original image dimensions\n",
    "                mask = cv2.resize(mask, (W, H))\n",
    "                \n",
    "                # Ensure the mask is binary (0 or 255)\n",
    "                _, binary_mask = cv2.threshold(mask, 128, 255, cv2.THRESH_BINARY)\n",
    "                \n",
    "                # Apply the mask to the original image using bitwise operations\n",
    "                masked_img = cv2.bitwise_and(img, img, mask=binary_mask.astype(np.uint8))\n",
    "                \n",
    "                # Combine with the filtered image\n",
    "                filtered_img = cv2.add(filtered_img, masked_img)\n",
    "        \n",
    "        # Save the filtered image\n",
    "        new_path = os.path.join(destination_folder, image_path.split('/')[-1])\n",
    "        cv2.imwrite(new_path, filtered_img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ccf9a350-d304-4b4f-9f75-966c1e69e2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_folder = '/home/jupyter-yixuan/project_machine_learning/angry_bears/image3'\n",
    "destination_folder = '/home/jupyter-yixuan/project_machine_learning/angry_bears/mask333'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ce95d303-8e01-4778-8854-51dcdf643722",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-yixuan/.local/lib/python3.10/site-packages/ultralytics/nn/tasks.py:336: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(file, map_location='cpu'), file  # load\n",
      "\n",
      "0: 448x640 1 Bear, 4.6ms\n",
      "Speed: 0.3ms preprocess, 4.6ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 Bear, 4.9ms\n",
      "Speed: 0.3ms preprocess, 4.9ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Bear, 5.3ms\n",
      "Speed: 0.3ms preprocess, 5.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x544 1 Bear, 5.5ms\n",
      "Speed: 0.3ms preprocess, 5.5ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 4.9ms\n",
      "Speed: 0.2ms preprocess, 4.9ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 1 Bear, 5.5ms\n",
      "Speed: 0.2ms preprocess, 5.5ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x512 1 Bear, 5.3ms\n",
      "Speed: 0.2ms preprocess, 5.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x448 1 Bear, 8.2ms\n",
      "Speed: 0.2ms preprocess, 8.2ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x544 1 Bear, 6.2ms\n",
      "Speed: 0.3ms preprocess, 6.2ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 4.7ms\n",
      "Speed: 0.2ms preprocess, 4.7ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 4.9ms\n",
      "Speed: 0.3ms preprocess, 4.9ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 6.7ms\n",
      "Speed: 0.2ms preprocess, 6.7ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter-yixuan/project_machine_learning/angry_bears/image3/32.jpeg\n",
      "/home/jupyter-yixuan/project_machine_learning/angry_bears/image3/72.jpeg\n",
      "/home/jupyter-yixuan/project_machine_learning/angry_bears/image3/14.jpeg\n",
      "/home/jupyter-yixuan/project_machine_learning/angry_bears/image3/26.jpeg\n",
      "/home/jupyter-yixuan/project_machine_learning/angry_bears/image3/70.jpeg\n",
      "/home/jupyter-yixuan/project_machine_learning/angry_bears/image3/89.jpeg\n",
      "/home/jupyter-yixuan/project_machine_learning/angry_bears/image3/52.jpeg\n",
      "/home/jupyter-yixuan/project_machine_learning/angry_bears/image3/4.jpeg\n",
      "/home/jupyter-yixuan/project_machine_learning/angry_bears/image3/82.jpeg\n",
      "/home/jupyter-yixuan/project_machine_learning/angry_bears/image3/34.jpeg\n",
      "/home/jupyter-yixuan/project_machine_learning/angry_bears/image3/29.jpeg\n",
      "/home/jupyter-yixuan/project_machine_learning/angry_bears/image3/1.jpeg\n",
      "/home/jupyter-yixuan/project_machine_learning/angry_bears/image3/22.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 544x640 1 Bear, 4.9ms\n",
      "Speed: 0.2ms preprocess, 4.9ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Bear, 5.1ms\n",
      "Speed: 0.2ms preprocess, 5.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 1 Bear, 5.1ms\n",
      "Speed: 0.2ms preprocess, 5.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Bear, 5.1ms\n",
      "Speed: 0.2ms preprocess, 5.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 Bear, 5.1ms\n",
      "Speed: 0.3ms preprocess, 5.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 5.1ms\n",
      "Speed: 0.2ms preprocess, 5.1ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 Bear, 4.9ms\n",
      "Speed: 0.3ms preprocess, 4.9ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 1 Bear, 4.8ms\n",
      "Speed: 0.2ms preprocess, 4.8ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x416 1 Bear, 5.1ms\n",
      "Speed: 0.2ms preprocess, 5.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 5.0ms\n",
      "Speed: 0.2ms preprocess, 5.0ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 4.7ms\n",
      "Speed: 0.2ms preprocess, 4.7ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 1 Bear, 5.2ms\n",
      "Speed: 0.2ms preprocess, 5.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 320x640 1 Bear, 5.2ms\n",
      "Speed: 0.2ms preprocess, 5.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 512x640 1 Bear, 5.0ms\n",
      "Speed: 0.2ms preprocess, 5.0ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 544x640 1 Bear, 5.1ms\n",
      "Speed: 0.2ms preprocess, 5.1ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 1 Bear, 4.7ms\n",
      "Speed: 0.2ms preprocess, 4.7ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 1 Bear, 5.0ms\n",
      "Speed: 0.2ms preprocess, 5.0ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Bear, 5.1ms\n",
      "Speed: 0.2ms preprocess, 5.1ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 1 Bear, 5.1ms\n",
      "Speed: 0.2ms preprocess, 5.1ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 1 Bear, 4.7ms\n",
      "Speed: 0.2ms preprocess, 4.7ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x480 1 Bear, 5.1ms\n",
      "Speed: 0.2ms preprocess, 5.1ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter-yixuan/project_machine_learning/angry_bears/image3/56.jpeg\n",
      "/home/jupyter-yixuan/project_machine_learning/angry_bears/image3/84.jpeg\n",
      "/home/jupyter-yixuan/project_machine_learning/angry_bears/image3/42.jpeg\n",
      "/home/jupyter-yixuan/project_machine_learning/angry_bears/image3/19.jpeg\n",
      "/home/jupyter-yixuan/project_machine_learning/angry_bears/image3/27.jpeg\n",
      "/home/jupyter-yixuan/project_machine_learning/angry_bears/image3/76.jpeg\n",
      "/home/jupyter-yixuan/project_machine_learning/angry_bears/image3/69.jpeg\n",
      "/home/jupyter-yixuan/project_machine_learning/angry_bears/image3/33.jpeg\n",
      "/home/jupyter-yixuan/project_machine_learning/angry_bears/image3/92.jpeg\n",
      "/home/jupyter-yixuan/project_machine_learning/angry_bears/image3/59.jpeg\n",
      "/home/jupyter-yixuan/project_machine_learning/angry_bears/image3/12.jpeg\n",
      "/home/jupyter-yixuan/project_machine_learning/angry_bears/image3/60.jpeg\n",
      "/home/jupyter-yixuan/project_machine_learning/angry_bears/image3/5.jpeg\n",
      "/home/jupyter-yixuan/project_machine_learning/angry_bears/image3/20.jpeg\n",
      "/home/jupyter-yixuan/project_machine_learning/angry_bears/image3/87.jpeg\n",
      "/home/jupyter-yixuan/project_machine_learning/angry_bears/image3/39.jpeg\n",
      "/home/jupyter-yixuan/project_machine_learning/angry_bears/image3/47.jpeg\n",
      "/home/jupyter-yixuan/project_machine_learning/angry_bears/image3/66.jpeg\n",
      "/home/jupyter-yixuan/project_machine_learning/angry_bears/image3/81.jpeg\n",
      "/home/jupyter-yixuan/project_machine_learning/angry_bears/image3/64.jpeg\n",
      "/home/jupyter-yixuan/project_machine_learning/angry_bears/image3/50.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 640x640 1 Bear, 4.9ms\n",
      "Speed: 0.3ms preprocess, 4.9ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 1 Bear, 5.2ms\n",
      "Speed: 0.2ms preprocess, 5.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x448 1 Bear, 5.2ms\n",
      "Speed: 0.2ms preprocess, 5.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x416 1 Bear, 5.2ms\n",
      "Speed: 0.2ms preprocess, 5.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 Bears, 5.2ms\n",
      "Speed: 0.2ms preprocess, 5.2ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 1 Bear, 4.8ms\n",
      "Speed: 0.2ms preprocess, 4.8ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 5.0ms\n",
      "Speed: 0.2ms preprocess, 5.0ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 1 Bear, 5.0ms\n",
      "Speed: 0.2ms preprocess, 5.0ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 Bear, 5.0ms\n",
      "Speed: 0.3ms preprocess, 5.0ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 4.8ms\n",
      "Speed: 0.2ms preprocess, 4.8ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 576x640 1 Bear, 5.3ms\n",
      "Speed: 0.3ms preprocess, 5.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 8.1ms\n",
      "Speed: 0.2ms preprocess, 8.1ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 4.8ms\n",
      "Speed: 0.2ms preprocess, 4.8ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Bear, 5.1ms\n",
      "Speed: 0.2ms preprocess, 5.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Bear, 4.8ms\n",
      "Speed: 0.2ms preprocess, 4.8ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 544x640 1 Bear, 5.1ms\n",
      "Speed: 0.3ms preprocess, 5.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x480 1 Bear, 4.8ms\n",
      "Speed: 0.2ms preprocess, 4.8ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 1 Bear, 5.0ms\n",
      "Speed: 0.2ms preprocess, 5.0ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 5.1ms\n",
      "Speed: 0.2ms preprocess, 5.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 4.8ms\n",
      "Speed: 0.3ms preprocess, 4.8ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter-yixuan/project_machine_learning/angry_bears/image3/65.jpeg\n",
      "/home/jupyter-yixuan/project_machine_learning/angry_bears/image3/53.jpeg\n",
      "/home/jupyter-yixuan/project_machine_learning/angry_bears/image3/74.jpeg\n",
      "/home/jupyter-yixuan/project_machine_learning/angry_bears/image3/73.jpeg\n",
      "/home/jupyter-yixuan/project_machine_learning/angry_bears/image3/54.jpeg\n",
      "/home/jupyter-yixuan/project_machine_learning/angry_bears/image3/18.jpeg\n",
      "/home/jupyter-yixuan/project_machine_learning/angry_bears/image3/83.jpeg\n",
      "/home/jupyter-yixuan/project_machine_learning/angry_bears/image3/63.jpeg\n",
      "/home/jupyter-yixuan/project_machine_learning/angry_bears/image3/61.jpeg\n",
      "/home/jupyter-yixuan/project_machine_learning/angry_bears/image3/85.jpeg\n",
      "/home/jupyter-yixuan/project_machine_learning/angry_bears/image3/15.jpeg\n",
      "/home/jupyter-yixuan/project_machine_learning/angry_bears/image3/40.jpeg\n",
      "/home/jupyter-yixuan/project_machine_learning/angry_bears/image3/91.jpeg\n",
      "/home/jupyter-yixuan/project_machine_learning/angry_bears/image3/36.jpeg\n",
      "/home/jupyter-yixuan/project_machine_learning/angry_bears/image3/45.jpeg\n",
      "/home/jupyter-yixuan/project_machine_learning/angry_bears/image3/94.jpeg\n",
      "/home/jupyter-yixuan/project_machine_learning/angry_bears/image3/75.jpeg\n",
      "/home/jupyter-yixuan/project_machine_learning/angry_bears/image3/35.jpeg\n",
      "/home/jupyter-yixuan/project_machine_learning/angry_bears/image3/10.jpeg\n",
      "/home/jupyter-yixuan/project_machine_learning/angry_bears/image3/3.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 416x640 1 Bear, 5.2ms\n",
      "Speed: 0.2ms preprocess, 5.2ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x576 1 Bear, 5.1ms\n",
      "Speed: 0.3ms preprocess, 5.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 Bear, 4.7ms\n",
      "Speed: 0.3ms preprocess, 4.7ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 5.1ms\n",
      "Speed: 0.2ms preprocess, 5.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x480 1 Bear, 5.0ms\n",
      "Speed: 0.2ms preprocess, 5.0ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Bear, 5.0ms\n",
      "Speed: 0.2ms preprocess, 5.0ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x544 1 Bear, 5.1ms\n",
      "Speed: 0.2ms preprocess, 5.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x448 1 Bear, 5.2ms\n",
      "Speed: 0.2ms preprocess, 5.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 1 Bear, 5.2ms\n",
      "Speed: 0.2ms preprocess, 5.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 5.0ms\n",
      "Speed: 0.2ms preprocess, 5.0ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 416x640 1 Bear, 4.8ms\n",
      "Speed: 0.2ms preprocess, 4.8ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 5.0ms\n",
      "Speed: 0.2ms preprocess, 5.0ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 4.6ms\n",
      "Speed: 0.2ms preprocess, 4.6ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 4.7ms\n",
      "Speed: 0.2ms preprocess, 4.7ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Bear, 5.2ms\n",
      "Speed: 0.3ms preprocess, 5.2ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 1 Bear, 5.9ms\n",
      "Speed: 0.2ms preprocess, 5.9ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x448 1 Bear, 6.0ms\n",
      "Speed: 0.3ms preprocess, 6.0ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Bear, 6.1ms\n",
      "Speed: 0.3ms preprocess, 6.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 2 Bears, 5.2ms\n",
      "Speed: 0.2ms preprocess, 5.2ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter-yixuan/project_machine_learning/angry_bears/image3/95.jpeg\n",
      "/home/jupyter-yixuan/project_machine_learning/angry_bears/image3/41.jpeg\n",
      "/home/jupyter-yixuan/project_machine_learning/angry_bears/image3/7.jpeg\n",
      "/home/jupyter-yixuan/project_machine_learning/angry_bears/image3/78.jpeg\n",
      "/home/jupyter-yixuan/project_machine_learning/angry_bears/image3/30.jpeg\n",
      "/home/jupyter-yixuan/project_machine_learning/angry_bears/image3/55.jpeg\n",
      "/home/jupyter-yixuan/project_machine_learning/angry_bears/image3/11.jpeg\n",
      "/home/jupyter-yixuan/project_machine_learning/angry_bears/image3/86.jpeg\n",
      "/home/jupyter-yixuan/project_machine_learning/angry_bears/image3/6.jpeg\n",
      "/home/jupyter-yixuan/project_machine_learning/angry_bears/image3/38.jpeg\n",
      "/home/jupyter-yixuan/project_machine_learning/angry_bears/image3/79.jpeg\n",
      "/home/jupyter-yixuan/project_machine_learning/angry_bears/image3/8.jpeg\n",
      "/home/jupyter-yixuan/project_machine_learning/angry_bears/image3/88.jpeg\n",
      "/home/jupyter-yixuan/project_machine_learning/angry_bears/image3/28.jpeg\n",
      "/home/jupyter-yixuan/project_machine_learning/angry_bears/image3/23.jpeg\n",
      "/home/jupyter-yixuan/project_machine_learning/angry_bears/image3/24.jpeg\n",
      "/home/jupyter-yixuan/project_machine_learning/angry_bears/image3/93.jpeg\n",
      "/home/jupyter-yixuan/project_machine_learning/angry_bears/image3/77.jpeg\n",
      "/home/jupyter-yixuan/project_machine_learning/angry_bears/image3/80.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 288x640 1 Bear, 22.4ms\n",
      "Speed: 0.2ms preprocess, 22.4ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 5.4ms\n",
      "Speed: 0.2ms preprocess, 5.4ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 Bear, 6.1ms\n",
      "Speed: 0.3ms preprocess, 6.1ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 Bear, 5.1ms\n",
      "Speed: 0.2ms preprocess, 5.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 1 Bear, 5.2ms\n",
      "Speed: 0.2ms preprocess, 5.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 5.2ms\n",
      "Speed: 0.2ms preprocess, 5.2ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 1 Bear, 5.0ms\n",
      "Speed: 0.2ms preprocess, 5.0ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 2 Bears, 4.7ms\n",
      "Speed: 0.2ms preprocess, 4.7ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 416x640 1 Bear, 5.1ms\n",
      "Speed: 0.2ms preprocess, 5.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 5.0ms\n",
      "Speed: 0.2ms preprocess, 5.0ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x416 1 Bear, 5.1ms\n",
      "Speed: 0.2ms preprocess, 5.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 5.9ms\n",
      "Speed: 0.2ms preprocess, 5.9ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 6.3ms\n",
      "Speed: 0.2ms preprocess, 6.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 4.9ms\n",
      "Speed: 0.2ms preprocess, 4.9ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x480 1 Bear, 5.7ms\n",
      "Speed: 0.2ms preprocess, 5.7ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 Bear, 7.9ms\n",
      "Speed: 0.3ms preprocess, 7.9ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 Bear, 9.1ms\n",
      "Speed: 0.3ms preprocess, 9.1ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter-yixuan/project_machine_learning/angry_bears/image3/2.jpeg\n",
      "/home/jupyter-yixuan/project_machine_learning/angry_bears/image3/67.jpeg\n",
      "/home/jupyter-yixuan/project_machine_learning/angry_bears/image3/58.jpeg\n",
      "/home/jupyter-yixuan/project_machine_learning/angry_bears/image3/37.jpeg\n",
      "/home/jupyter-yixuan/project_machine_learning/angry_bears/image3/43.jpeg\n",
      "/home/jupyter-yixuan/project_machine_learning/angry_bears/image3/21.jpeg\n",
      "/home/jupyter-yixuan/project_machine_learning/angry_bears/image3/17.jpeg\n",
      "/home/jupyter-yixuan/project_machine_learning/angry_bears/image3/9.jpeg\n",
      "/home/jupyter-yixuan/project_machine_learning/angry_bears/image3/96.jpeg\n",
      "/home/jupyter-yixuan/project_machine_learning/angry_bears/image3/90.jpeg\n",
      "/home/jupyter-yixuan/project_machine_learning/angry_bears/image3/31.jpeg\n",
      "/home/jupyter-yixuan/project_machine_learning/angry_bears/image3/51.jpeg\n",
      "/home/jupyter-yixuan/project_machine_learning/angry_bears/image3/46.jpeg\n",
      "/home/jupyter-yixuan/project_machine_learning/angry_bears/image3/13.jpeg\n",
      "/home/jupyter-yixuan/project_machine_learning/angry_bears/image3/71.jpeg\n",
      "/home/jupyter-yixuan/project_machine_learning/angry_bears/image3/49.jpeg\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "model_path = '/home/jupyter-yixuan/project_machine_learning/runs/segment/train9/weights/last.pt'\n",
    "\n",
    "# Load the YOLO model\n",
    "model = YOLO(model_path)\n",
    "\n",
    "\n",
    "# image_path = '/home/jupyter-yixuan/project_machine_learning/angry_bears/images/1.jpeg'\n",
    "for image_path in os.listdir(original_folder):\n",
    "    extension = image_path.split('.')[-1]\n",
    "    if extension == 'jpeg':    \n",
    "        image_path = os.path.join(original_folder, image_path)\n",
    "        # Load the original image\n",
    "        print(image_path)\n",
    "        img = cv2.imread(image_path)\n",
    "        H, W, _ = img.shape\n",
    "        \n",
    "        # Perform inference to get the masks\n",
    "        results = model(img)\n",
    "    \n",
    "        if results is not None:\n",
    "            # Initialize a blank canvas to store the filtered result\n",
    "            filtered_img = np.zeros_like(img)\n",
    "            \n",
    "            for result in results:\n",
    "                for j, mask in enumerate(result.masks.data):\n",
    "                    # Convert the mask to a numpy array and scale to 255\n",
    "                    mask = mask.cpu().numpy() * 255\n",
    "                    \n",
    "                    # Resize the mask to match the original image dimensions\n",
    "                    mask = cv2.resize(mask, (W, H))\n",
    "                    \n",
    "                    # Ensure the mask is binary (0 or 255)\n",
    "                    _, binary_mask = cv2.threshold(mask, 128, 255, cv2.THRESH_BINARY)\n",
    "                    \n",
    "                    # Apply the mask to the original image using bitwise operations\n",
    "                    masked_img = cv2.bitwise_and(img, img, mask=binary_mask.astype(np.uint8))\n",
    "                    \n",
    "                    # Combine with the filtered image\n",
    "                    filtered_img = cv2.add(filtered_img, masked_img)\n",
    "            \n",
    "            # Save the filtered image\n",
    "            new_path = os.path.join(destination_folder, image_path.split('/')[-1])\n",
    "            cv2.imwrite(new_path, filtered_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8a1d21-90cf-42d0-bd7b-36d1cc6f4225",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
